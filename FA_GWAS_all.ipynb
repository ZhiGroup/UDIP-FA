{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FA-GWAS Analysis Notebook\n",
    "\n",
    "This notebook contains the comprehensive analysis pipeline for FA (Fractional Anisotropy) GWAS studies.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Loading and Preprocessing](#data-loading)\n",
    "2. [UDIP-FA Age and Sex Prediction](#udip-prediction)\n",
    "3. [PerD Result Visualization](#perd-visualization)\n",
    "4. [GWAS Analysis](#gwas-analysis)\n",
    "5. [PRS Analysis](#prs-analysis)\n",
    "6. [Results and Visualization](#results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Statistical analysis\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from scipy.stats import rankdata\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import mpmath as mp\n",
    "mp.mp.dps = 100  # Increase precision for small p-values\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import lightgbm as lgb\n",
    "import umap\n",
    "\n",
    "# Neuroimaging\n",
    "from nilearn import datasets, plotting, image\n",
    "\n",
    "# Parallel processing\n",
    "from multiprocessing import Pool\n",
    "from joblib import Parallel, delayed\n",
    "from subprocess import check_output, STDOUT\n",
    "from itertools import zip_longest, combinations\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from matplotlib.colors import LinearSegmentedColormap, to_hex, Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "# Configuration\n",
    "FA_figure_dir = '/data/xzhao14/FA_figures'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\nThis section handles the loading and initial preprocessing of FA data and phenotype information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "473ad"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "from pathlib import Path\n",
    "import os, numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from subprocess import check_output, STDOUT\n",
    "from itertools import zip_longest\n",
    "from glob import glob\n",
    "import pickle\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "FA_figure_dir='/data/xzhao14/FA_figures'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import rankdata\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import mpmath as mp\n",
    "\n",
    "# Increase mpmath precision to capture extremely small p-values\n",
    "mp.mp.dps = 100\n",
    "\n",
    "def upper_f_pvalue(f_stat, df1, df2):\n",
    "    \"\"\"\n",
    "    Compute P(F(df1, df2) > f_stat) via the regularized upper incomplete beta:\n",
    "      P = I_z(a, b) from z to 1,\n",
    "    where a = df1/2, b = df2/2, and z = (df1 * f_stat) / (df1 * f_stat + df2).\n",
    "    \"\"\"\n",
    "    a = mp.mpf(df1) / 2\n",
    "    b = mp.mpf(df2) / 2\n",
    "    z = (mp.mpf(df1) * mp.mpf(f_stat)) / (mp.mpf(df1) * mp.mpf(f_stat) + mp.mpf(df2))\n",
    "    return mp.betainc(a, b, z, 1, regularized=True)\n",
    "\n",
    "def analyze_udip_and_csf(udip_csv, csf_csv, output_csv, umap_fig):\n",
    "    # --- load & merge ---\n",
    "    udip = pd.read_csv(udip_csv)\n",
    "    csf  = pd.read_csv(csf_csv)\n",
    "    udip.columns = udip.columns.astype(str)\n",
    "    csf.columns  = csf.columns.astype(str)\n",
    "    merged = pd.merge(\n",
    "        udip, csf,\n",
    "        left_on=udip.columns[0],\n",
    "        right_on=csf.columns[0]\n",
    "    )\n",
    "\n",
    "    # Prepare X and Y\n",
    "    X = merged[udip.columns[2:]].values\n",
    "    Y = merged[csf.columns[1:]].values\n",
    "    m = Y.shape[1]  # number of tests for Bonferroni\n",
    "\n",
    "    results = []\n",
    "    for i, region in enumerate(csf.columns[1:]):\n",
    "        y = Y[:, i]\n",
    "\n",
    "        # 1) Compute R²\n",
    "        lr = LinearRegression().fit(X, y)\n",
    "        r2 = r2_score(y, lr.predict(X))\n",
    "\n",
    "        # 2) Fit OLS to get F-statistic\n",
    "        X_sm  = sm.add_constant(X)\n",
    "        model = sm.OLS(y, X_sm).fit()\n",
    "        f_stat = float(model.fvalue)\n",
    "        df1, df2 = int(model.df_model), int(model.df_resid)\n",
    "\n",
    "        # 3) Compute high-precision p-value\n",
    "        p_mp = upper_f_pvalue(f_stat, df1, df2)\n",
    "        print(p_mp)\n",
    "        # 4) Format output: if p_mp is zero at this precision, report \"<1e-<precision>\"\n",
    "        if p_mp == 0:\n",
    "            raw_str = f\"<1e-{mp.mp.dps}\"\n",
    "            bonf_str = raw_str\n",
    "        else:\n",
    "            raw_str  = mp.nstr(p_mp, 20)                    # 20 significant digits\n",
    "            bonf_str = mp.nstr(min(p_mp * m, mp.mpf(1)), 20)\n",
    "\n",
    "        results.append({\n",
    "            'Brain Region':        region,\n",
    "            'R2 Score':            f\"{r2:.4f}\",\n",
    "            'Raw P Value':         raw_str,\n",
    "            'Bonferroni P Value':  bonf_str\n",
    "        })\n",
    "\n",
    "    # Save results\n",
    "    pd.DataFrame(results).to_csv(output_csv, index=False)\n",
    "\n",
    "    # --- UMAP plotting ---\n",
    "    csf_mean    = Y.mean(axis=1)\n",
    "    csf_percent = rankdata(csf_mean) / len(csf_mean) * 100\n",
    "    emb = umap.UMAP(n_components=2, random_state=42).fit_transform(X)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sc = ax.scatter(emb[:,0], emb[:,1], c=csf_percent, cmap='hot', s=2)\n",
    "    cbar = plt.colorbar(sc, ax=ax, fraction=0.02, pad=0.04)\n",
    "    cbar.set_label('CSF percentile', rotation=270, labelpad=15)\n",
    "    cbar.set_ticks([0, 50, 100])\n",
    "    ax.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(umap_fig, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# example call\n",
    "analyze_udip_and_csf(\n",
    "    udip_csv='/data/xzhao14/FA_all_phenotype.csv',\n",
    "    csf_csv='/data/xzhao14/FA_value.csv',\n",
    "    output_csv='/data/xzhao14/outputs/FA_r2_scores.csv',\n",
    "    umap_fig='/data/xzhao14/outputs/udip_umap.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "1fb00"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from decimal import Decimal, getcontext\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "import decimal\n",
    "from scipy.stats import f\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "def analyze_udip_and_csf(udip_csv, csf_csv, output_r2_csv, umap_fig_path):\n",
    "    \"\"\"\n",
    "    Perform multivariate regression between UDIP features and CSF volumes,\n",
    "    compute R² scores and overall p-values, and visualize UMAP embedding \n",
    "    colored by percentile-ranked mean CSF. Includes Bonferroni correction.\n",
    "\n",
    "    Parameters:\n",
    "    - udip_csv: str, path to the UDIP CSV file\n",
    "    - csf_csv: str, path to the CSF percentile CSV file\n",
    "    - output_r2_csv: str, path to save R² and p-values\n",
    "    - umap_fig_path: str, path to save UMAP figure\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load input CSVs\n",
    "    udip_df = pd.read_csv(udip_csv)\n",
    "    csf_df = pd.read_csv(csf_csv)\n",
    "\n",
    "    # Ensure consistent column naming\n",
    "    udip_df.columns = udip_df.columns.astype(str)\n",
    "    csf_df.columns = csf_df.columns.astype(str)\n",
    "\n",
    "    # Merge on subject ID (first column of each file)\n",
    "    merged_df = pd.merge(udip_df, csf_df, left_on=udip_df.columns[0], right_on=csf_df.columns[0])\n",
    "    print(merged_df.shape)\n",
    "\n",
    "    # Extract matrices\n",
    "    feature_cols = udip_df.columns[2:]  # skip ID columns\n",
    "    csf_cols = csf_df.columns[1:]       # skip ID column\n",
    "\n",
    "    X = merged_df[feature_cols].values\n",
    "    Y = merged_df[csf_cols].values\n",
    "\n",
    "    # Compute R² and p-value for each brain region\n",
    "    getcontext().prec = 200  # set high precision for Decimal\n",
    "\n",
    "    results = []\n",
    "    raw_pvals = []\n",
    "\n",
    "    for i, region in enumerate(csf_cols):\n",
    "        y = Y[:, i]\n",
    "\n",
    "        # R² from sklearn\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X, y)\n",
    "        y_pred = lr.predict(X)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "\n",
    "        # statsmodels fit\n",
    "        X_sm = sm.add_constant(X)\n",
    "        model = sm.OLS(y, X_sm).fit()\n",
    "\n",
    "        # manually compute the right tail of the F distribution (i.e., p-value)\n",
    "        f_stat = float(model.fvalue)\n",
    "        df1 = int(model.df_model)\n",
    "        df2 = int(model.df_resid)\n",
    "\n",
    "        # compute p-value using scipy\n",
    "        pval_float = f.sf(f_stat, df1, df2)\n",
    "\n",
    "        # convert to Decimal to preserve tiny values\n",
    "        pval_decimal = Decimal(str(pval_float))\n",
    "\n",
    "        raw_pvals.append(pval_decimal)\n",
    "\n",
    "        results.append({\n",
    "            'Brain Region': region,\n",
    "            'R2 Score': round(r2, 4),\n",
    "            'Raw P Value': pval_decimal\n",
    "        })\n",
    "\n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Bonferroni correction\n",
    "    num_tests = len(results_df)\n",
    "    results_df['Bonferroni P Value'] = results_df['Raw P Value'].apply(lambda p: min(p * num_tests, 1.0))\n",
    "\n",
    "    # Format p-values and R² for clarity\n",
    "    results_df['R2 Score'] = results_df['R2 Score'].apply(lambda x: f\"{x:.4f}\")\n",
    "    #results_df['Raw P Value'] = results_df['Raw P Value'].apply(lambda x: format(x, '.15e'))\n",
    "    #results_df['Bonferroni P Value'] = results_df['Bonferroni P Value'].apply(lambda x: format(x, '.15e'))\n",
    "    getcontext().prec = 200\n",
    "\n",
    "    results_df['Raw P Value'] = results_df['Raw P Value'].apply(lambda x: format(Decimal(str(x)), '.15e'))\n",
    "    results_df['Bonferroni P Value'] = results_df['Bonferroni P Value'].apply(lambda x: format(Decimal(str(x)), '.15e'))\n",
    "    # Save results to CSV\n",
    "    results_df.to_csv(output_r2_csv, index=False)\n",
    "\n",
    "    # Compute mean CSF and convert to percentile\n",
    "    csf_mean = Y.mean(axis=1)\n",
    "    csf_percentile = rankdata(csf_mean, method='average') / len(csf_mean) * 100\n",
    "\n",
    "    # UMAP dimensionality reduction\n",
    "    # Fit UMAP to reduce dimensionality to 2D\n",
    "# Fit UMAP to reduce high-dimensional data to 2D\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    embedding = reducer.fit_transform(X)  # X: your feature matrix\n",
    "\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # Use 'hot' colormap: dark (low values) -> yellow/white (high values)\n",
    "    sc = ax.scatter(embedding[:, 0], embedding[:, 1], c=csf_percentile, cmap='hot', s=2)\n",
    "\n",
    "    # Add vertical colorbar\n",
    "    cbar = plt.colorbar(sc, ax=ax, fraction=0.02, pad=0.04)\n",
    "    cbar.set_label('Vol. of Mean FA values (percentile)', rotation=270, labelpad=15)  # Label on the right\n",
    "    cbar.ax.tick_params(labelsize=10)\n",
    "    cbar.set_ticks([0, 50, 100])  # Customize tick marks\n",
    "\n",
    "    # Remove axis lines and ticks\n",
    "    ax.set_title('UMAP of UDIPs colored by CSF percentile', fontsize=14)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Optimize layout and save\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(umap_fig_path, dpi=300)\n",
    "    plt.show()\n",
    "plt.close()\n",
    "### #\n",
    "analyze_udip_and_csf(\n",
    "    udip_csv='/data/xzhao14/FA_all_phenotype.csv',\n",
    "    csf_csv='/data/xzhao14/FA_value.csv',\n",
    "    output_r2_csv='/data/xzhao14/outputs/FA_r2_scores.csv',\n",
    "    umap_fig_path='/data/xzhao14/outputs/udip_umap.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "89b36"
   },
   "outputs": [],
   "source": [
    "## calculate the mean FA R2 and association between\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "FA_r2=pd.read_csv('/data/xzhao14/outputs/FA_r2_scores.csv')\n",
    "np.mean(FA_r2['R2 Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualization of the mean FA in different brain regions\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "579de"
   },
   "source": [
    "### Using the UDIP-FA to predicted the age and sex \n",
    "- using the feature matrix to predicted the age and sex\n",
    "- using the cluster methods to cluster the 128 feature \n",
    "- using five fold cross-validation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "eab23"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score, accuracy_score, classification_report, roc_curve\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import KFold\n",
    "def cluster_features(X, max_clusters=10, visualize=True, random_state=42,save_dir=\"/data/xzhao14/outputs\"):\n",
    "    \"\"\"\n",
    "    Cluster the feature matrix and visualize the clustering results.\n",
    "\n",
    "    Parameters:\n",
    "    - X: np.array, shape (n_samples, n_features), the original feature matrix.\n",
    "    - max_clusters: int, maximum number of clusters to test (used to determine optimal number of clusters).\n",
    "    - visualize: bool, whether to visualize the clustering heatmap.\n",
    "    - random_state: int, random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - labels: cluster labels, shape (n_features,).\n",
    "    - best_k: optimal number of clusters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Transpose matrix to cluster features\n",
    "    X_T = X.T\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_T)\n",
    "\n",
    "    # Automatically determine the optimal number of clusters using silhouette score\n",
    "    best_k = 2\n",
    "    best_score = -1\n",
    "    for k in range(2, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=random_state)\n",
    "        labels = kmeans.fit_predict(X_scaled)\n",
    "        score = silhouette_score(X_scaled, labels)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "\n",
    "    # Perform final clustering with optimal number of clusters\n",
    "    kmeans_final = KMeans(n_clusters=best_k, random_state=random_state)\n",
    "    final_labels = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "    # Visualize clustering results\n",
    "    if visualize:\n",
    "        sorted_idx = np.argsort(final_labels)\n",
    "        sorted_X = X_scaled[sorted_idx, :]\n",
    "        sorted_labels = final_labels[sorted_idx]\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(sorted_X, cmap='vlag', cbar=True, xticklabels=False, yticklabels=False)\n",
    "        plt.title(f'Feature Clustering Heatmap (k={best_k})', fontsize=14)\n",
    "        plt.xlabel('Samples', fontsize=12)\n",
    "        plt.ylabel('Clustered Features', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        #plt.savefig(\"feature_clustering_heatmap.pdf\")\n",
    "        plt.savefig(os.path.join(save_dir, \"feature_clustering_heatmap.pdf\"))\n",
    "        plt.close()\n",
    "\n",
    "    return final_labels, best_k\n",
    "\n",
    "\n",
    "def predict_age_gender(X, age, gender, n_splits=5, random_state=42, save_dir=\"/data/xzhao14/outputs\"):\n",
    "    \"\"\"\n",
    "    Perform regression (age) and classification (gender) prediction from feature matrix using 5-fold cross validation.\n",
    "\n",
    "    Parameters:\n",
    "    - X: np.array, shape (n_samples, n_features), feature matrix.\n",
    "    - age: np.array, shape (n_samples,), target ages.\n",
    "    - gender: np.array, shape (n_samples,), binary gender labels (0/1).\n",
    "    - n_splits: int, number of cross validation folds.\n",
    "    - random_state: int, random seed for reproducibility.\n",
    "    - save_dir: str, directory to save output figures.\n",
    "\n",
    "    Returns:\n",
    "    - age_metrics: dict, containing MAE and Pearson r (mean and std) for SVR and ElasticNet.\n",
    "    - gender_metrics: dict, containing Accuracy, AUC (mean and std), and classification report for SVC.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Lists to store fold-wise metrics\n",
    "    svr_mae_list, svr_pearson_list = [], []\n",
    "    enet_mae_list, enet_pearson_list = [], []\n",
    "    svc_auc_list, svc_acc_list = [], []\n",
    "\n",
    "    # To aggregate predictions for plotting\n",
    "    agg_age_true_svr, agg_age_pred_svr = [], []\n",
    "    agg_age_true_enet, agg_age_pred_enet = [], []\n",
    "    agg_gender_true, agg_gender_prob, agg_gender_pred = [], [], []\n",
    "    \n",
    "    # For ROC curve: store fpr and tpr for each fold\n",
    "    svc_fpr_list, svc_tpr_list = [], []\n",
    "\n",
    "    # Set up 5-fold cross validation\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Split fold data\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        age_train, age_test = age[train_index], age[test_index]\n",
    "        gender_train, gender_test = gender[train_index], gender[test_index]\n",
    "        \n",
    "        # Standardize features using training data only\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # --- Age prediction using SVR ---\n",
    "        svr = SVR()\n",
    "        svr.fit(X_train_scaled, age_train)\n",
    "        age_pred_svr = svr.predict(X_test_scaled)\n",
    "        mae_svr = mean_absolute_error(age_test, age_pred_svr)\n",
    "        pearson_r_svr, _ = pearsonr(age_test, age_pred_svr)\n",
    "        \n",
    "        svr_mae_list.append(mae_svr)\n",
    "        svr_pearson_list.append(pearson_r_svr)\n",
    "        agg_age_true_svr.extend(age_test)\n",
    "        agg_age_pred_svr.extend(age_pred_svr)\n",
    "        \n",
    "        # --- Age prediction using ElasticNet ---\n",
    "        enet = ElasticNet(random_state=random_state)\n",
    "        enet.fit(X_train_scaled, age_train)\n",
    "        age_pred_enet = enet.predict(X_test_scaled)\n",
    "        mae_enet = mean_absolute_error(age_test, age_pred_enet)\n",
    "        pearson_r_enet, _ = pearsonr(age_test, age_pred_enet)\n",
    "        \n",
    "        enet_mae_list.append(mae_enet)\n",
    "        enet_pearson_list.append(pearson_r_enet)\n",
    "        agg_age_true_enet.extend(age_test)\n",
    "        agg_age_pred_enet.extend(age_pred_enet)\n",
    "        \n",
    "        # --- Gender prediction using SVC ---\n",
    "        svc = SVC(probability=True, random_state=random_state)\n",
    "        svc.fit(X_train_scaled, gender_train)\n",
    "        gender_pred = svc.predict(X_test_scaled)\n",
    "        gender_prob = svc.predict_proba(X_test_scaled)[:, 1]\n",
    "        auc = roc_auc_score(gender_test, gender_prob)\n",
    "        acc = accuracy_score(gender_test, gender_pred)\n",
    "        \n",
    "        svc_auc_list.append(auc)\n",
    "        svc_acc_list.append(acc)\n",
    "        agg_gender_true.extend(gender_test)\n",
    "        agg_gender_prob.extend(gender_prob)\n",
    "        agg_gender_pred.extend(gender_pred)\n",
    "        \n",
    "        # Compute ROC curve for this fold and store it\n",
    "        fpr, tpr, _ = roc_curve(gender_test, gender_prob)\n",
    "        svc_fpr_list.append(fpr)\n",
    "        svc_tpr_list.append(tpr)\n",
    "    \n",
    "    # Compute mean and standard deviation for regression metrics\n",
    "    svr_mae_mean, svr_mae_std = np.mean(svr_mae_list), np.std(svr_mae_list)\n",
    "    svr_pearson_mean, svr_pearson_std = np.mean(svr_pearson_list), np.std(svr_pearson_list)\n",
    "    enet_mae_mean, enet_mae_std = np.mean(enet_mae_list), np.std(enet_mae_list)\n",
    "    enet_pearson_mean, enet_pearson_std = np.mean(enet_pearson_list), np.std(enet_pearson_list)\n",
    "    \n",
    "    # Compute mean and std for classification metrics\n",
    "    svc_auc_mean, svc_auc_std = np.mean(svc_auc_list), np.std(svc_auc_list)\n",
    "    svc_acc_mean, svc_acc_std = np.mean(svc_acc_list), np.std(svc_acc_list)\n",
    "    \n",
    "    # ----- Plotting Regression Results for SVR -----\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(agg_age_true_svr, agg_age_pred_svr, alpha=0.6, label='SVR Predictions')\n",
    "    x_min, x_max = min(agg_age_true_svr), max(agg_age_true_svr)\n",
    "    plt.plot([x_min, x_max], [x_min, x_max], 'r--', label='Ideal')\n",
    "    plt.xlabel('True Age')\n",
    "    plt.ylabel('Predicted Age')\n",
    "    plt.title('SVR Age Prediction (5-Fold CV)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    annotation_text = (f\"MAE = {svr_mae_mean:.2f} ± {svr_mae_std:.2f}\\n\"\n",
    "                       f\"Pearson r = {svr_pearson_mean:.2f} ± {svr_pearson_std:.2f}\")\n",
    "    plt.annotate(annotation_text, xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                 fontsize=10, verticalalignment='top')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"svr_age_prediction_cv.pdf\"))\n",
    "    plt.close()\n",
    "    \n",
    "    # ----- Plotting Regression Results for ElasticNet -----\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(agg_age_true_enet, agg_age_pred_enet, alpha=0.6, label='ElasticNet Predictions')\n",
    "    x_min, x_max = min(agg_age_true_enet), max(agg_age_true_enet)\n",
    "    plt.plot([x_min, x_max], [x_min, x_max], 'r--', label='Ideal')\n",
    "    plt.xlabel('True Age')\n",
    "    plt.ylabel('Predicted Age')\n",
    "    plt.title('ElasticNet Age Prediction (5-Fold CV)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    annotation_text = (f\"MAE = {enet_mae_mean:.2f} ± {enet_mae_std:.2f}\\n\"\n",
    "                       f\"Pearson r = {enet_pearson_mean:.2f} ± {enet_pearson_std:.2f}\")\n",
    "    plt.annotate(annotation_text, xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                 fontsize=10, verticalalignment='top')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"elasticnet_age_prediction_cv.pdf\"))\n",
    "    plt.close()\n",
    "    # ----- Plotting Mean ROC Curve for SVC -----\n",
    "    # Create a common FPR axis and interpolate each fold's TPR to it\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    tprs_interp = []\n",
    "    for fpr, tpr in zip(svc_fpr_list, svc_tpr_list):\n",
    "        interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs_interp.append(interp_tpr)\n",
    "    tprs_interp = np.array(tprs_interp)\n",
    "    mean_tpr = tprs_interp.mean(axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    std_tpr = tprs_interp.std(axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(mean_fpr, mean_tpr, label=f'Mean ROC (AUC = {svc_auc_mean:.2f} ± {svc_auc_std:.2f})', lw=2)\n",
    "    plt.fill_between(mean_fpr, mean_tpr - std_tpr, mean_tpr + std_tpr, color='grey',\n",
    "                     alpha=0.3, label='± 1 std. dev.')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Mean ROC Curve for Gender Classification (5-Fold CV)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    annotation_text = f\"Accuracy = {svc_acc_mean:.2f} ± {svc_acc_std:.2f}\"\n",
    "    plt.annotate(annotation_text, xy=(0.05, 0.05), xycoords='axes fraction', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"svc_gender_roc_cv.pdf\"))\n",
    "    plt.close()\n",
    "    \n",
    "    # Prepare metrics dictionaries\n",
    "    age_metrics = {\n",
    "        'SVR': {'MAE_mean': svr_mae_mean, 'MAE_std': svr_mae_std,\n",
    "                'Pearson_r_mean': svr_pearson_mean, 'Pearson_r_std': svr_pearson_std},\n",
    "        'ElasticNet': {'MAE_mean': enet_mae_mean, 'MAE_std': enet_mae_std,\n",
    "                       'Pearson_r_mean': enet_pearson_mean, 'Pearson_r_std': enet_pearson_std}\n",
    "    }\n",
    "    \n",
    "    gender_metrics = {\n",
    "        'SVC': {'Accuracy_mean': svc_acc_mean, 'Accuracy_std': svc_acc_std,\n",
    "                'AUC_mean': svc_auc_mean, 'AUC_std': svc_auc_std,\n",
    "                'Classification_Report': classification_report(np.array(agg_gender_true), \n",
    "                                                               np.array(agg_gender_pred), output_dict=True)}\n",
    "    }\n",
    "    \n",
    "    return age_metrics, gender_metrics\n",
    "\n",
    "# Example Usage:\n",
    "data_path='/data/xzhao14/FA_all_phenotype.csv'\n",
    "covar_file = pd.read_csv('/data/xzhao14/PRS_covar.csv',delim_whitespace=True)\n",
    "phenotype_file = pd.read_csv(data_path)\n",
    "common_iids = phenotype_file['IID'].isin(covar_file['IID'])\n",
    "sample_ids = phenotype_file.iloc[:, :2]  # First two columns are IDs\n",
    "features = phenotype_file.iloc[:, 2:].values  # Remaining columns are features\n",
    "labels, optimal_k = cluster_features(features, max_clusters=40, visualize=True,save_dir='/data/xzhao14/FA_age_prediction')\n",
    "# phenotype_file = phenotype_file[common_iids]\n",
    "# # reorder the phenotype files ##\n",
    "# covar_file = covar_file.set_index('IID')\n",
    "# phenotype_file = phenotype_file.set_index('IID')\n",
    "# covar_file = covar_file.loc[phenotype_file.index]\n",
    "# covar_file = covar_file.reset_index()\n",
    "# phenotype_file = phenotype_file.reset_index()\n",
    "# sex=covar_file['SEX'].values\n",
    "# age=covar_file['AGE'].values\n",
    "# features = phenotype_file.iloc[:, 2:].values\n",
    "# age_metrics, gender_metrics = predict_age_gender(features, age, sex,save_dir='/data/xzhao14/FA_age_prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "f8193"
   },
   "source": [
    "## PerD result visualization \n",
    "- For different tissue(include CSF,GM and WM)\n",
    "- For different regions of WM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "66019"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "\n",
    "\n",
    "def load_nifti(file_path):\n",
    "    \"\"\"Load a NIfTI file and return the nibabel image object\"\"\"\n",
    "    return nib.load(file_path)\n",
    "\n",
    "\n",
    "def load_region_info(txt_file):\n",
    "    \"\"\"\n",
    "    Load brain region info file, assuming it contains three columns:\n",
    "      Column 1: numerical label in the segmentation template\n",
    "      Column 2: abbreviated brain region name\n",
    "      Column 3: full brain region name\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(txt_file, sep='\\t', header=None, names=['value', 'abbr', 'full_name'], index_col=None)\n",
    "    print(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_ks_statistic(tmap_data, seg_data, region_df, top_n_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Compute Kolmogorov-Smirnov (K-S) statistic for each brain region\n",
    "    and return the results sorted by statistic value.\n",
    "    \"\"\"\n",
    "    tmap_data = np.abs(tmap_data)\n",
    "    \n",
    "    # Retain only non-zero regions in mask\n",
    "    masked_tmap = tmap_data[seg_data > 0]\n",
    "    \n",
    "    # Total number of voxels in the mask\n",
    "    total_voxels = masked_tmap.size\n",
    "    \n",
    "    # Number of top_n voxels\n",
    "    top_n = int(total_voxels * top_n_ratio)\n",
    "    \n",
    "    # Sort t-values in descending order and get top_n indices\n",
    "    sorted_indices = np.argsort(-masked_tmap)\n",
    "    top_n_indices = sorted_indices[:top_n]\n",
    "    \n",
    "    total_top_n_voxels = len(top_n_indices)\n",
    "    \n",
    "    ks_stats = {}\n",
    "    \n",
    "    # Iterate over each unique brain region\n",
    "    unique_regions = np.unique(seg_data)\n",
    "    for reg_val in unique_regions:\n",
    "        if reg_val == 0:\n",
    "            continue\n",
    "        \n",
    "        mask = seg_data == reg_val\n",
    "        region_voxels = np.sum(mask)\n",
    "        if region_voxels == 0:\n",
    "            continue\n",
    "        \n",
    "        region_t_values = tmap_data[mask]\n",
    "        \n",
    "        region_top_n_voxels = np.sum(region_t_values >= masked_tmap[top_n_indices[-1]])\n",
    "        \n",
    "        ks_stat = (region_top_n_voxels / region_voxels) - (total_top_n_voxels / total_voxels)\n",
    "        ks_stats[reg_val] = ks_stat\n",
    "    \n",
    "    # Match region names and sort\n",
    "    ks_results = []\n",
    "    for reg_val, ks_stat in ks_stats.items():\n",
    "        row = region_df[region_df['value'] == reg_val]\n",
    "        region_name = row.iloc[0]['abbr'] if not row.empty else f\"Region {reg_val}\"\n",
    "        ks_results.append((reg_val, region_name, ks_stat))\n",
    "    \n",
    "    ks_results.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    return ks_results\n",
    "\n",
    "\n",
    "def generate_ks_nifti(ks_results, seg_data, seg_img):\n",
    "    \"\"\"\n",
    "    Generate a new NIfTI file where voxel values in each region \n",
    "    are set to the region's K-S statistic\n",
    "    \"\"\"\n",
    "    ks_map = np.zeros_like(seg_data)\n",
    "\n",
    "    for reg_val, _, ks_stat in ks_results:\n",
    "        ks_map[seg_data == reg_val] = ks_stat\n",
    "\n",
    "    return nib.Nifti1Image(ks_map, affine=seg_img.affine, header=seg_img.header)\n",
    "\n",
    "\n",
    "def visualize_top_ks_regions(ks_results, seg_data, seg_img):\n",
    "    \"\"\"\n",
    "    Visualize the top 3 brain regions with the highest K-S statistics\n",
    "    \"\"\"\n",
    "    top_3_regions = ks_results[:3]\n",
    "    top_3_map = np.zeros_like(seg_data)\n",
    "    for reg_val, _, ks_stat in top_3_regions:\n",
    "        top_3_map[seg_data == reg_val] = ks_stat\n",
    "\n",
    "    top_3_img = nib.Nifti1Image(top_3_map, affine=seg_img.affine, header=seg_img.header)\n",
    "\n",
    "    # You can uncomment this to use a white matter atlas background\n",
    "    # jhu_wm = datasets.fetch_atlas_jhu_dti_81()\n",
    "    # wm_bg_img = nib.load(jhu_wm.maps)\n",
    "\n",
    "    view = plotting.view_img(\n",
    "        top_3_img,\n",
    "        # bg_img=wm_bg_img,\n",
    "        symmetric_cmap=False,\n",
    "        vmin=0,\n",
    "        draw_cross=False,\n",
    "        cmap=\"coolwarm\",\n",
    "        black_bg=True\n",
    "        # title=\"White Matter\"\n",
    "    )\n",
    "    display(view)\n",
    "\n",
    "\n",
    "def save_ks_to_csv(ks_results, output_csv):\n",
    "    \"\"\"\n",
    "    Save K-S statistics to CSV file\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(ks_results, columns=[\"Region Value\", \"Region Name\", \"KS Statistic\"])\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"K-S statistics saved to: {output_csv}\")\n",
    "\n",
    "\n",
    "def main(tmap_file, seg_file, region_txt, output_file, output_csv):\n",
    "    # Load NIfTI files\n",
    "    tmap_img = load_nifti(tmap_file)\n",
    "    seg_img = load_nifti(seg_file)\n",
    "\n",
    "    # Get data and use absolute t-values\n",
    "    tmap_data = np.abs(tmap_img.get_fdata())\n",
    "    seg_data = seg_img.get_fdata()\n",
    "\n",
    "    # Load region info\n",
    "    region_df = load_region_info(region_txt)\n",
    "\n",
    "    # Compute and sort K-S statistics\n",
    "    ks_results = compute_ks_statistic(tmap_data, seg_data, region_df, top_n_ratio=0.01)\n",
    "\n",
    "    # Save results\n",
    "    save_ks_to_csv(ks_results, output_csv)\n",
    "\n",
    "    print(\"\\nK-S statistics (sorted in descending order):\")\n",
    "    region_d = []\n",
    "    ks_stat = []\n",
    "    for _, region, ks_val in ks_results:\n",
    "        significance = \"\"\n",
    "        if ks_val >= 0.1:\n",
    "            significance = \"*** (highly significant)\"\n",
    "        elif ks_val >= 0.05:\n",
    "            significance = \"** (significant)\"\n",
    "        elif ks_val >= 0.02:\n",
    "            significance = \"* (possibly significant)\"\n",
    "        print(f\"{region}: {ks_val:.4f} {significance}\")\n",
    "\n",
    "    # Uncomment the following if you want to save a K-S NIfTI map\n",
    "    # ks_img = generate_ks_nifti(ks_results, seg_data, seg_img)\n",
    "    # nib.save(ks_img, output_file)\n",
    "    # print(f\"K-S NIfTI map saved to: {output_file}\")\n",
    "\n",
    "    # Uncomment the following to visualize the top 3 regions\n",
    "    # visualize_top_ks_regions(ks_results, seg_data, seg_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "e3805"
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from nilearn import plotting, image, datasets\n",
    "from IPython.display import display\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from difflib import SequenceMatcher\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from joblib import Parallel, delayed\n",
    "# read the templated \n",
    "seg_file='/data484_2/xzhao14/FA_atlas/WM_ICBM_WMPM_1mm.nii'\n",
    "endopheno='/data484_2/xzhao14/FA_perd'\n",
    "out_put_files='/data484_2/xzhao14/FA_perd/PerD_region/'\n",
    "csv_out='/data484_2/xzhao14/FA_perd/PerD_region/csv_pair/'\n",
    "region_txt='/data484_2/xzhao14/FA_atlas/LabelLookupTable.txt'\n",
    "endo_num=list(range(128))\n",
    "for i in endo_num:\n",
    "   # i=10\n",
    "    tem_nii_name='paired_ttest_T1_'+str(i)+'.nii.gz'\n",
    "    tmap_file=os.path.join(endopheno,tem_nii_name)\n",
    "    output_file=os.path.join(out_put_files,str(i)+'.nii.gz')\n",
    "    output_csv=os.path.join(csv_out,str(i)+'.csv')\n",
    "    main(tmap_file,seg_file,region_txt,output_file,output_csv)\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "a0d86"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nilearn import plotting\n",
    "from tqdm import tqdm\n",
    "\n",
    "def permutation_test_on_roi_csv(tmap_path, mask_path, output_csv_path, txt_file, n_perm=5000):\n",
    "    \"\"\"\n",
    "    Perform a non-parametric permutation test on a t-map and ROI mask, apply Bonferroni correction \n",
    "    to the resulting p-values, and save the ROI information (including the corrected p-values in \n",
    "    scientific notation) into a CSV file. The text file should contain three columns:\n",
    "        'value' (mask value),\n",
    "        'abbr' (abbreviation), and \n",
    "        'full_name' (full name).\n",
    "        \n",
    "    Steps:\n",
    "      1. Load the t-map and mask NIfTI images and convert the t-map to absolute values.\n",
    "      2. Load ROI information from the text file and create a mapping from mask values to ROI abbreviations.\n",
    "      3. For each ROI in the mask, compute the observed statistic (mean absolute t-value).\n",
    "      4. Perform permutation testing by shuffling t-values within the mask to build a null distribution.\n",
    "      5. Calculate raw p-values for each ROI using the correction (r+1)/(n_perm+1), where r is the \n",
    "         number of permutations with statistic >= observed statistic.\n",
    "      6. Apply Bonferroni correction: multiply each raw p-value by the number of tests and cap at 1.\n",
    "      7. Compile the ROI mask value, abbreviation, full name, observed mean, and corrected p-value \n",
    "         (displayed in scientific notation) into a CSV file.\n",
    "      \n",
    "    Parameters:\n",
    "      tmap_path: Path to the t-map NIfTI file (t-values will be converted to absolute values).\n",
    "      mask_path: Path to the ROI mask NIfTI file (background should be 0, ROIs are labeled with non-zero integers).\n",
    "      output_csv_path: Path to save the output CSV file.\n",
    "      txt_file: Path to the text file containing ROI labels and names.\n",
    "      n_perm: Number of permutations to perform (default 5000).\n",
    "      \n",
    "    Returns:\n",
    "      results_df: A pandas DataFrame containing the ROI information and corrected p-values.\n",
    "      p_values: A dictionary with ROI abbreviations as keys and their corresponding corrected p-values as values.\n",
    "      df: The DataFrame read from the ROI text file.\n",
    "    \"\"\"\n",
    "    # 1. Load NIfTI images and convert the t-map to absolute values\n",
    "    tmap_img = nib.load(tmap_path)\n",
    "    mask_img = nib.load(mask_path)\n",
    "\n",
    "    tmap_data = np.abs(tmap_img.get_fdata())\n",
    "    mask_data = mask_img.get_fdata()\n",
    "\n",
    "    if tmap_data.shape != mask_data.shape:\n",
    "        raise ValueError(\"t-map and mask dimensions do not match!\")\n",
    "\n",
    "    # 2. Load ROI names from the text file\n",
    "    df = pd.read_csv(txt_file, sep='\\t', header=None, names=['value', 'abbr', 'full_name'], index_col=None)\n",
    "    print(\"Loaded ROI information from the text file:\\n\", df)\n",
    "\n",
    "    # Create a mapping from mask value to ROI abbreviation\n",
    "    roi_mapping = df.set_index('value')['abbr'].to_dict()\n",
    "\n",
    "    # 3. Preprocessing: Flatten data and retrieve non-zero mask indices\n",
    "    tmap_flat = tmap_data.flatten()\n",
    "    mask_flat = mask_data.flatten()\n",
    "\n",
    "    # Get indices of voxels within the mask (non-zero values)\n",
    "    mask_indices = np.where(mask_flat != 0)[0]\n",
    "    tvals_mask = tmap_flat[mask_indices]\n",
    "\n",
    "    # Get all unique non-zero ROI labels (excluding background 0)\n",
    "    roi_labels = np.unique(mask_flat[mask_flat != 0])\n",
    "    print(\"Detected ROI labels from mask:\", roi_labels)\n",
    "\n",
    "    # 4. Compute the observed statistic (mean absolute t-value) for each ROI\n",
    "    roi_indices_dict = {}  # Dictionary to store indices for each ROI in the flattened array\n",
    "    observed_stats = {}    # Dictionary to store the observed statistic for each ROI\n",
    "\n",
    "    for roi in roi_labels:\n",
    "        roi_voxel_indices = np.where(mask_flat == roi)[0]\n",
    "        roi_indices_dict[roi] = roi_voxel_indices\n",
    "        observed_stats[roi] = np.median(tmap_flat[roi_voxel_indices])\n",
    "\n",
    "    # 5. Perform permutation testing: Build the null distribution\n",
    "    perm_stats = {roi: np.zeros(n_perm) for roi in roi_labels}\n",
    "    print(\"Starting permutation testing with %d iterations...\" % n_perm)\n",
    "    for i in tqdm(range(n_perm)):\n",
    "        # Randomly shuffle t-values within the mask\n",
    "        permuted_tvals = np.random.permutation(tvals_mask)\n",
    "        permuted_tmap_flat = tmap_flat.copy()\n",
    "        permuted_tmap_flat[mask_indices] = permuted_tvals\n",
    "\n",
    "        # Compute the statistic for each ROI using the permuted data\n",
    "        for roi in roi_labels:\n",
    "            roi_voxel_indices = roi_indices_dict[roi]\n",
    "            perm_stats[roi][i] = np.mean(permuted_tmap_flat[roi_voxel_indices])\n",
    "\n",
    "    # 6. Calculate raw p-values with correction and apply Bonferroni correction\n",
    "    p_values = {}\n",
    "    cor_p_value={}\n",
    "    num_tests = len(roi_labels)\n",
    "    for roi in roi_labels:\n",
    "        obs = observed_stats[roi]\n",
    "        perm_array = perm_stats[roi]\n",
    "        # Use (r + 1) / (n_perm + 1) to avoid p == 0\n",
    "        raw_p_val = (np.sum(perm_array >= obs) + 1) / (n_perm + 1)\n",
    "        # Bonferroni correction: multiply by the number of tests and cap at 1\n",
    "        corr_p_val = raw_p_val * num_tests\n",
    "        if corr_p_val > 1:\n",
    "            corr_p_val = 1.0\n",
    "        # Use ROI abbreviation if available; otherwise, convert to string\n",
    "        roi_key = roi_mapping.get(roi, str(roi))\n",
    "        p_values[roi_key] = raw_p_val\n",
    "        cor_p_value[roi_key]=corr_p_val\n",
    "        # Display in scientific notation\n",
    "        print(\"ROI %s (mask value %s): observed mean = %.4f, raw p = %.4f, corrected p = %s\" % \n",
    "              (roi_key, roi, obs, raw_p_val, f\"{corr_p_val:.2e}\"))\n",
    "\n",
    "    # 7. Construct the results DataFrame and write to a CSV file with scientific notation for p-values\n",
    "    result_list = []\n",
    "    for roi in roi_labels:\n",
    "        abbreviation = roi_mapping.get(roi, str(roi))\n",
    "        # Retrieve the full name from the DataFrame; if not found, set as 'NA'\n",
    "        full_name_series = df.loc[df['value'] == roi, 'full_name']\n",
    "        full_name = full_name_series.iloc[0] if not full_name_series.empty else 'NA'\n",
    "        obs_mean = observed_stats[roi]\n",
    "        # p_value is already Bonferroni-corrected and formatted in scientific notation\n",
    "        corr_p_val = cor_p_value[abbreviation]\n",
    "        p_val = p_values[abbreviation]\n",
    "        result_list.append({\n",
    "            'ROI_value': roi,\n",
    "            'abbr': abbreviation,\n",
    "            'full_name': full_name,\n",
    "            'observed_mean': obs_mean,\n",
    "            'p_value_raw':f\"{p_val:.6e}\",\n",
    "            'p_value': f\"{corr_p_val:.6e}\"\n",
    "        })\n",
    "    results_df = pd.DataFrame(result_list)\n",
    "    results_df.to_csv(output_csv_path, index=False)\n",
    "    print(\"P-value CSV file saved to:\", output_csv_path)\n",
    "\n",
    "    return results_df, p_values, df\n",
    "\n",
    "# Example usage: Adjust file paths as needed\n",
    "if __name__ == '__main__':\n",
    "    root_dir = '/data484_2/xzhao14/FA_perd'\n",
    "    out_dir = '/data484_2/xzhao14/FA_perd/sig_region_statistic'\n",
    "    all_per_files = [f for f in os.listdir(root_dir) if f.endswith('.nii.gz')]\n",
    "    for i in range(len(all_per_files)):\n",
    "        tmap_path = os.path.join(root_dir, all_per_files[i])\n",
    "        print(\"Processing:\", tmap_path)\n",
    "        out_name = str(i) + '_sigre.csv'\n",
    "        mask_path = '/data484_2/xzhao14/FA_atlas/WM_ICBM_WMPM_1mm.nii'\n",
    "        output_csv_path = os.path.join(out_dir, out_name)\n",
    "        txt_file = '/data484_2/xzhao14/FA_atlas/LabelLookupTable.txt'\n",
    "        results_df, p_values, roi_df = permutation_test_on_roi_csv(tmap_path, mask_path, output_csv_path, txt_file, n_perm=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "248c5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import mannwhitneyu\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def process_and_visualize_ks_data(folder_path, output_csv=\"merged_ks_statistics.csv\"):\n",
    "    \"\"\"\n",
    "    Process multiple CSV files containing KS statistics, merge them into one DataFrame,\n",
    "    and generate both a heatmap and a boxplot with statistical annotations.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the directory of CSV files.\n",
    "        output_csv (str): Filename for the merged CSV output.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The merged DataFrame of KS statistics.\n",
    "    \"\"\"\n",
    "    # Verify that the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        raise FileNotFoundError(f\"The folder '{folder_path}' does not exist.\")\n",
    "\n",
    "    # List all CSV files in the folder\n",
    "    all_files = [\n",
    "        os.path.join(folder_path, f)\n",
    "        for f in os.listdir(folder_path)\n",
    "        if f.endswith('.csv')\n",
    "    ]\n",
    "    if not all_files:\n",
    "        raise ValueError(f\"No CSV files found in '{folder_path}'.\")\n",
    "\n",
    "    # Read and label each CSV, then collect into a list\n",
    "    all_data = []\n",
    "    file_labels = []\n",
    "    for file in all_files:\n",
    "        df = pd.read_csv(file)\n",
    "        label = 'UDIP-FA_' + os.path.splitext(os.path.basename(file))[0]\n",
    "        df[\"File\"] = label  # Tag each row with its source file\n",
    "        all_data.append(df)\n",
    "        file_labels.append(label)\n",
    "\n",
    "    # Concatenate all data into one DataFrame and save to CSV\n",
    "    merged_data = pd.concat(all_data, ignore_index=True)\n",
    "    merged_data.to_csv(output_csv, index=False)\n",
    "    print(f\"Merged data saved to: {output_csv}\")\n",
    "\n",
    "    # Pivot to create a heatmap data matrix: rows = files, cols = regions\n",
    "    heatmap_data = merged_data.pivot_table(\n",
    "        index=\"File\", columns=\"Region Name\", values=\"KS Statistic\"\n",
    "    )\n",
    "    # Ensure rows follow the original file order\n",
    "    heatmap_data = heatmap_data.reindex(file_labels)\n",
    "    print(heatmap_data)\n",
    "\n",
    "    # Draw the heatmap\n",
    "    plt.figure(figsize=(\n",
    "        max(10, len(heatmap_data.columns) * 2),\n",
    "        max(10, len(heatmap_data.index) / 5)\n",
    "    ))\n",
    "    sns.heatmap(\n",
    "        heatmap_data,\n",
    "        cmap=\"coolwarm\",\n",
    "        annot=False,\n",
    "        cbar=True,\n",
    "        linewidths=0.1\n",
    "    )\n",
    "    plt.title(\"KS Statistic Distribution Heatmap\", fontsize=16)\n",
    "    plt.ylabel(\"Files\", fontsize=12)\n",
    "    plt.xlabel(\"Regions\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(FA_figure_dir, \"region_heatmap_sig.pdf\"),\n",
    "        format=\"pdf\",\n",
    "        bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    # Prepare for boxplot: clean style and custom palette\n",
    "    sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "    nature_palette = [\"#88CCEE\", \"#44AA99\", \"#117733\"]  # Extend if more groups\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.boxplot(\n",
    "        data=merged_data,\n",
    "        x=\"Region Name\",\n",
    "        y=\"KS Statistic\",\n",
    "        palette=nature_palette,\n",
    "        width=0.6,\n",
    "        fliersize=4\n",
    "    )\n",
    "    plt.title(\"Boxplot of KS Statistic for CSF, GM, and WM\", fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Regions\", fontsize=14)\n",
    "    plt.ylabel(\"KS Statistic\", fontsize=14)\n",
    "\n",
    "    # Determine group order and y-axis limits for annotation\n",
    "    group_order = [tick.get_text() for tick in ax.get_xticklabels()]\n",
    "    y_max = merged_data.groupby(\"Region Name\")[\"KS Statistic\"].max()\n",
    "    y_min = merged_data[\"KS Statistic\"].min()\n",
    "    y_range = merged_data[\"KS Statistic\"].max() - y_min\n",
    "    offset = y_range * 0.05  # Space above boxes for significance bars\n",
    "\n",
    "    # Perform pairwise comparisons and annotate significance\n",
    "    for idx, (i, j) in enumerate(combinations(range(len(group_order)), 2)):\n",
    "        grp1 = group_order[i]\n",
    "        grp2 = group_order[j]\n",
    "        data1 = merged_data.loc[merged_data[\"Region Name\"] == grp1, \"KS Statistic\"]\n",
    "        data2 = merged_data.loc[merged_data[\"Region Name\"] == grp2, \"KS Statistic\"]\n",
    "\n",
    "        stat, p = mannwhitneyu(data1, data2, alternative='two-sided')\n",
    "\n",
    "        # Decide significance marker\n",
    "        if p < 0.001:\n",
    "            marker = '***'\n",
    "        elif p < 0.01:\n",
    "            marker = '**'\n",
    "        elif p < 0.05:\n",
    "            marker = '*'\n",
    "        else:\n",
    "            marker = 'ns'\n",
    "\n",
    "        # Compute height for the annotation line\n",
    "        h = max(y_max[grp1], y_max[grp2]) + offset + idx * offset\n",
    "        x1, x2 = i, j\n",
    "        x_center = (x1 + x2) / 2.0\n",
    "\n",
    "        # Draw the significance bar\n",
    "        ax.plot(\n",
    "            [x1, x1, x2, x2],\n",
    "            [h - offset/2, h, h, h - offset/2],\n",
    "            lw=1.5, c='k'\n",
    "        )\n",
    "        # Place the text marker\n",
    "        ax.text(\n",
    "            x_center, h,\n",
    "            marker,\n",
    "            ha='center',\n",
    "            va='bottom',\n",
    "            fontsize=14,\n",
    "            color='k'\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(FA_figure_dir, \"region_boxplot_sig.pdf\"),\n",
    "        format=\"pdf\",\n",
    "        bbox_inches=\"tight\"\n",
    "    )\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "# Example usage:\n",
    "folder_path = \"/data484_2/xzhao14/FA_perd/GM_WM_seg/\"\n",
    "merged_df = process_and_visualize_ks_data(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "82255"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_prob):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for binary classification.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): True binary labels (0/1).\n",
    "        y_pred (array-like): Predicted binary labels (0/1).\n",
    "        y_prob (array-like): Predicted probabilities for the positive class.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing AUC, accuracy, sensitivity, specificity,\n",
    "              precision, Youden index, and F1 score.\n",
    "    \"\"\"\n",
    "    sensitivity = recall_score(y_true, y_pred)\n",
    "    # Compute specificity, guarding against division-by-zero\n",
    "    specificity = (\n",
    "        np.sum((y_true == 0) & (y_pred == 0)) / np.sum(y_true == 0)\n",
    "        if np.sum(y_true == 0) > 0 else 0\n",
    "    )\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    youden_index = sensitivity + specificity - 1\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return {\n",
    "        'AUC': auc,\n",
    "        'Accuracy': accuracy,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'Precision': precision,\n",
    "        'Youden Index': youden_index,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "def classify_and_evaluate(csv_path, txt_path, n_iterations=10, balance_strategy='undersample', negative_ratio=3):\n",
    "    \"\"\"\n",
    "    Load data, train LightGBM classifiers with specified balance strategy,\n",
    "    and compute evaluation metrics over multiple iterations.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file containing feature matrix and IDs.\n",
    "        txt_path (str): Path to the text file listing disease sample IDs (column 'eid').\n",
    "        n_iterations (int): Number of repeated train/test splits.\n",
    "        balance_strategy (str): Sampling strategy: 'undersample' or 'full_weighted'.\n",
    "        negative_ratio (int): Ratio of healthy to disease samples when undersampling.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Aggregated metrics over all iterations.\n",
    "        list: List of top-3 feature importance Series from each iteration.\n",
    "    \"\"\"\n",
    "    # 1. Read the main data CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Assume the first two columns are ID fields\n",
    "    df_ids = df.iloc[:, :2]\n",
    "\n",
    "    # Read the disease sample IDs from txt file (must contain 'eid' column)\n",
    "    disease_ids = pd.read_csv(txt_path)\n",
    "    disease_ids = set(disease_ids['eid'])\n",
    "\n",
    "    # Mark disease vs healthy samples\n",
    "    df['is_disease'] = df_ids.iloc[:, 0].isin(disease_ids).astype(int)\n",
    "    df_disease = df[df['is_disease'] == 1]\n",
    "    df_healthy = df[df['is_disease'] == 0]\n",
    "    print(f\"Disease samples: {len(df_disease)}, Healthy samples: {len(df_healthy)}\")\n",
    "    if df_disease.empty or df_healthy.empty:\n",
    "        raise ValueError(\"Not enough disease or healthy samples for classification.\")\n",
    "\n",
    "    all_metrics = []        # store metrics for each iteration\n",
    "    top_features_list = []  # store top-3 features for each iteration\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        print(f\"\\nIteration {i + 1}/{n_iterations}\")\n",
    "\n",
    "        # Balance the data according to the chosen strategy\n",
    "        if balance_strategy == 'undersample':\n",
    "            # Randomly sample healthy controls at specified ratio\n",
    "            sampled_healthy = df_healthy.sample(\n",
    "                n=len(df_disease) * negative_ratio, random_state=i\n",
    "            )\n",
    "            balanced_df = pd.concat([df_disease, sampled_healthy])\n",
    "        elif balance_strategy == 'full_weighted':\n",
    "            # Use all samples, applying class weights instead of undersampling\n",
    "            balanced_df = pd.concat([df_disease, df_healthy])\n",
    "        else:\n",
    "            raise ValueError(\"balance_strategy must be 'undersample' or 'full_weighted'\")\n",
    "\n",
    "        # Define features (columns 3 to second-last) and label\n",
    "        X = balanced_df.iloc[:, 2:-1]\n",
    "        y = balanced_df['is_disease']\n",
    "\n",
    "        # 2. Split into train/test sets with stratification\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.1, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        # 3. Train LightGBM with sampling weights or class weights\n",
    "        if balance_strategy == 'undersample':\n",
    "            # Assign higher weight to negative class in training\n",
    "            sample_weight = y_train.map({0: negative_ratio, 1: 1}).values\n",
    "            clf = lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "            clf.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "        else:  # full_weighted\n",
    "            # Compute scale_pos_weight for LightGBM\n",
    "            pos = np.sum(y_train == 1)\n",
    "            neg = np.sum(y_train == 0)\n",
    "            scale_pos_weight = neg / pos if pos > 0 else 1.0\n",
    "            clf = lgb.LGBMClassifier(\n",
    "                random_state=42, verbose=-1, scale_pos_weight=scale_pos_weight\n",
    "            )\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "        # Extract and record top-3 features by importance\n",
    "        feature_importances = pd.Series(clf.feature_importances_, index=X_train.columns)\n",
    "        top3_features = feature_importances.sort_values(ascending=False).head(3)\n",
    "        print(\"Top 3 features in this iteration:\")\n",
    "        print(top3_features)\n",
    "        top_features_list.append(top3_features)\n",
    "\n",
    "        # 4. Evaluate on test set (threshold 0.5)\n",
    "        y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_prob >= 0.5).astype(int)\n",
    "        fold_metrics = calculate_metrics(y_test.values, y_pred, y_prob)\n",
    "        print(\"Metrics:\", fold_metrics)\n",
    "        all_metrics.append(fold_metrics)\n",
    "\n",
    "    # Aggregate metrics across iterations\n",
    "    aggregated_metrics = pd.DataFrame(all_metrics)\n",
    "    print(\"\\nAggregated Metrics across iterations:\")\n",
    "    print(aggregated_metrics.describe())\n",
    "\n",
    "    return aggregated_metrics, top_features_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Directory containing disease sample ID files\n",
    "    disease_sample_dir = '/data/xzhao14/UKB_sample_ICD10'\n",
    "    all_disease_files = os.listdir(disease_sample_dir)\n",
    "    csv_file = \"/data/xzhao14/FA_all_phenotype.csv\"\n",
    "    print(\"Disease files:\", all_disease_files)\n",
    "\n",
    "    # Optionally skip the first file\n",
    "    all_disease_files = all_disease_files[1:]\n",
    "    res_all = []\n",
    "    top_features_all = {}\n",
    "\n",
    "    # Choose balancing strategy and ratio\n",
    "    balance_strategy = 'undersample'  # or 'full_weighted'\n",
    "    negative_ratio = 2  # healthy-to-disease ratio when undersampling\n",
    "\n",
    "    for file_name in all_disease_files:\n",
    "        print(f\"\\nProcessing: {file_name}\")\n",
    "        txt_file = os.path.join(disease_sample_dir, file_name)\n",
    "        try:\n",
    "            metrics, top_features = classify_and_evaluate(\n",
    "                csv_file,\n",
    "                txt_file,\n",
    "                n_iterations=10,\n",
    "                balance_strategy=balance_strategy,\n",
    "                negative_ratio=negative_ratio\n",
    "            )\n",
    "            res_all.append(metrics)\n",
    "            top_features_all[file_name] = top_features\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "    # (Optional) Save results to a pickle file\n",
    "    # output_pickle = \"/data/xzhao14/FA_classification_results.pkl\"\n",
    "    # with open(output_pickle, 'wb') as f:\n",
    "    #     pickle.dump({'metrics': res_all, 'top_features': top_features_all}, f)\n",
    "    # print(f\"Results saved to {output_pickle}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDIP-FA Age and Sex Prediction\n",
    "\nUsing the UDIP-FA feature matrix to predict age and sex using clustering methods and five-fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "17a20"
   },
   "outputs": [],
   "source": [
    "## test \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/data/xzhao14/FA_all_phenotype.csv\")\n",
    "\n",
    "\n",
    "id_cols = df.iloc[:, :2]\n",
    "id_cols.to_csv(\"/data484_2/xzhao14/FA_all_sample.txt\", sep='\\t', index=False)\n",
    "\n",
    "for col in df.columns[2:]:\n",
    "    #combined = pd.concat([id_cols, df[[col]]], axis=1)\n",
    "    combined = pd.concat([id_cols, df[[col]].rename(columns={col: 'PHENO'})], axis=1)\n",
    "    combined.to_csv(f\"/data484_2/xzhao14/FA_fea_all/Feature_{col}.txt\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "72be9"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/data/xzhao14/FA_all_phenotype.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "27cb8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_prob):\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics.\n",
    "    \n",
    "    Parameters:\n",
    "      y_true: Ground truth labels.\n",
    "      y_pred: Predicted labels.\n",
    "      y_prob: Predicted probabilities for the positive class.\n",
    "    \n",
    "    Returns:\n",
    "      A dictionary with AUC, Accuracy, Sensitivity, Specificity, Precision, Youden Index, and F1 Score.\n",
    "    \"\"\"\n",
    "    sensitivity = recall_score(y_true, y_pred)\n",
    "    specificity = np.sum((y_true == 0) & (y_pred == 0)) / np.sum(y_true == 0) if np.sum(y_true == 0) > 0 else 0\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    youden_index = sensitivity + specificity - 1\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return {\n",
    "        'AUC': auc,\n",
    "        'Accuracy': accuracy,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'Precision': precision,\n",
    "        'Youden Index': youden_index,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "def stochastic_downsample(train_df, minority_label=1, random_state=42):\n",
    "    \"\"\"\n",
    "    Balance training data using the stochastic downsampling strategy.\n",
    "    \n",
    "    Steps:\n",
    "      1) Split the training DataFrame into minority and majority classes.\n",
    "      2) Calculate the ratio: r = |B| / |A|, where B is the minority class and A is the majority.\n",
    "      3) Randomly sample floor(r * |A|) samples from the majority class.\n",
    "      4) Combine the downsampled majority samples with the full minority class and shuffle.\n",
    "    \n",
    "    Parameters:\n",
    "      train_df: DataFrame containing features and a label column 'is_disease'.\n",
    "      minority_label: The label of the minority class (default is 1 for disease samples).\n",
    "      random_state: Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "      A balanced DataFrame.\n",
    "    \"\"\"\n",
    "    df_minority = train_df[train_df['is_disease'] == minority_label]\n",
    "    df_majority = train_df[train_df['is_disease'] != minority_label]\n",
    "\n",
    "    # Assume majority (A) and minority (B); if minority count is actually larger, swap them.\n",
    "    A = df_majority\n",
    "    B = df_minority\n",
    "    if len(df_minority) > len(df_majority):\n",
    "        A, B = B, A\n",
    "\n",
    "    # Calculate the downsampling ratio r\n",
    "    r = len(B) / len(A) if len(A) > 0 else 1.0\n",
    "    downsample_size = int(np.floor(r * len(A)))\n",
    "    if downsample_size < 1:\n",
    "        raise ValueError(\"The downsampled majority class has less than 1 sample. Data too imbalanced or training set too small.\")\n",
    "    \n",
    "    # Downsample A and combine with B\n",
    "    A_prime = A.sample(n=downsample_size, random_state=random_state)\n",
    "    balanced_df = pd.concat([A_prime, B]).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    return balanced_df\n",
    "\n",
    "def classify_with_stochastic_downsampling(csv_path, disease_file, n_splits=10, random_state=42):\n",
    "    \"\"\"\n",
    "    For one disease file, use the stochastic downsampling strategy to balance samples,\n",
    "    then perform stratified 10-fold cross-validation to train and evaluate the model.\n",
    "    \n",
    "    Parameters:\n",
    "      csv_path: Path to the CSV file containing all sample features. \n",
    "                Assumes the first two columns are IDs and remaining columns are features.\n",
    "      disease_file: Path to a disease sample file that contains a column named 'eid'.\n",
    "      n_splits: Number of cross-validation splits (default is 10).\n",
    "      random_state: Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "      A tuple (metrics_df, top_features_list) where:\n",
    "         - metrics_df is a DataFrame with aggregated metrics across folds.\n",
    "         - top_features_list is a list of the top 3 feature importances from each fold.\n",
    "    \"\"\"\n",
    "    # Read the full feature CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df_ids = df.iloc[:, :2]  # The first two columns are assumed to be IDs\n",
    "\n",
    "    # Read disease sample IDs from the disease file (expects a column 'eid')\n",
    "    disease_ids = pd.read_csv(disease_file)['eid'].unique().tolist()\n",
    "    # Mark disease samples based on whether the first column of df_ids is in disease_ids\n",
    "    df['is_disease'] = df_ids.iloc[:, 0].isin(disease_ids).astype(int)\n",
    "\n",
    "    # Print the distribution of classes\n",
    "    num_disease = df['is_disease'].sum()\n",
    "    num_healthy = len(df) - num_disease\n",
    "    print(f\"Processing disease file: {os.path.basename(disease_file)}\")\n",
    "    print(f\"  Disease samples: {num_disease}, Healthy samples: {num_healthy}\")\n",
    "    if num_disease == 0 or num_healthy == 0:\n",
    "        raise ValueError(\"Not enough disease or healthy samples to perform classification.\")\n",
    "\n",
    "    # Construct feature matrix and labels.\n",
    "    # Assume features are from the third column until the last (which is 'is_disease').\n",
    "    X = df.iloc[:, 2:-1]\n",
    "    y = df['is_disease'].values\n",
    "\n",
    "    # Set up stratified K-Fold cross-validation.\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    all_metrics = []\n",
    "    top_features_list = []\n",
    "\n",
    "    fold_idx = 0\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        fold_idx += 1\n",
    "        print(f\"\\nFold {fold_idx}/{n_splits}\")\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "\n",
    "        # Combine training features and labels into a single DataFrame.\n",
    "        train_df = pd.concat([X_train, pd.Series(y_train, name='is_disease')], axis=1)\n",
    "        # Apply the stochastic downsampling to balance the training data.\n",
    "        train_balanced = stochastic_downsample(train_df, minority_label=1, random_state=random_state + fold_idx)\n",
    "        X_train_balanced = train_balanced.drop(columns=['is_disease'])\n",
    "        y_train_balanced = train_balanced['is_disease']\n",
    "\n",
    "        # Train the LightGBM model using the balanced training set.\n",
    "        clf = lgb.LGBMClassifier(random_state=random_state + fold_idx, verbose=-1)\n",
    "        clf.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "        # Retrieve feature importance and record the top 3 features.\n",
    "        feature_importances = pd.Series(clf.feature_importances_, index=X_train_balanced.columns)\n",
    "        top3_features = feature_importances.sort_values(ascending=False).head(3)\n",
    "        print(\"Top 3 features in this fold:\")\n",
    "        print(top3_features)\n",
    "        top_features_list.append(top3_features)\n",
    "\n",
    "        # Evaluate on the original (imbalanced) test set.\n",
    "        y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_prob >= 0.5).astype(int)\n",
    "        fold_metrics = calculate_metrics(y_test, y_pred, y_prob)\n",
    "        print(\"Fold metrics:\", fold_metrics)\n",
    "        all_metrics.append(fold_metrics)\n",
    "\n",
    "    metrics_df = pd.DataFrame(all_metrics)\n",
    "    print(\"\\nAggregated Metrics across folds:\")\n",
    "    print(metrics_df.describe())\n",
    "\n",
    "    return metrics_df, top_features_list\n",
    "\n",
    "# ---------------- Main Process ----------------\n",
    "if __name__ == '__main__':\n",
    "    # Path to the feature CSV file (all samples and features)\n",
    "    csv_file = \"/data/xzhao14/FA_all_phenotype.csv\"\n",
    "    # Directory containing multiple disease sample CSV files\n",
    "    disease_sample_dir = '/data/xzhao14/UKB_sample_ICD10'\n",
    "    all_disease_files = os.listdir(disease_sample_dir)\n",
    "    all_disease_files =all_disease_files[1:] \n",
    "    print(\"Disease files:\", all_disease_files)\n",
    "    \n",
    "    # Dictionary to store the results for each disease file.\n",
    "    results = {}\n",
    "    for disease_filename in all_disease_files:\n",
    "        disease_file_path = os.path.join(disease_sample_dir, disease_filename)\n",
    "        try:\n",
    "            metrics, top_feats = classify_with_stochastic_downsampling(\n",
    "                csv_path=csv_file,\n",
    "                disease_file=disease_file_path,\n",
    "                n_splits=10,\n",
    "                random_state=42\n",
    "            )\n",
    "            results[disease_filename] = {\n",
    "                'metrics': metrics,\n",
    "                'top_features': top_feats\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {disease_filename}: {e}\")\n",
    "\n",
    "    # Optionally, you can save the results to a pickle file.\n",
    "    # output_pickle_file = \"/data/xzhao14/FA_classification_results.pkl\"\n",
    "    # with open(output_pickle_file, 'wb') as f:\n",
    "    #     pickle.dump(results, f)\n",
    "    # print(f\"\\nResults saved to {output_pickle_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "e2bbd"
   },
   "outputs": [],
   "source": [
    "### Importance visualization ###\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "def filter_mask(input_nii_path, values_to_keep, output_nii_path):\n",
    "    \"\"\"\n",
    "    Reads a NIfTI mask file, sets all voxel values that are not in the specified list to 0,\n",
    "    and then saves the modified image as a new NIfTI file.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_nii_path : str\n",
    "        Path to the input NIfTI mask file (.nii or .nii.gz).\n",
    "    values_to_keep : list\n",
    "        List of voxel values to retain; all other voxel values will be set to 0.\n",
    "    output_nii_path : str\n",
    "        Path to save the output NIfTI file (.nii or .nii.gz).\n",
    "    \"\"\"\n",
    "    # 1. Load the input NIfTI file\n",
    "    img = nib.load(input_nii_path)\n",
    "    data = img.get_fdata()  # Get the image data as a float array\n",
    "\n",
    "    # 2. Create a boolean mask that is True for voxels whose values are in the allowed list.\n",
    "    # Using np.isin should work correctly for multiple values.\n",
    "    mask = np.isin(data, values_to_keep)\n",
    "    \n",
    "    # Alternatively, if you experience issues with np.isin, you can build the mask with a loop:\n",
    "    # mask = np.zeros(data.shape, dtype=bool)\n",
    "    # for val in values_to_keep:\n",
    "    #     mask |= (data == val)\n",
    "    \n",
    "    # 3. Apply the mask to retain allowed values and set all other voxels to 0.\n",
    "    # Multiplying by the boolean mask preserves the original values where mask==True.\n",
    "    data_filtered = data * mask\n",
    "\n",
    "    # 4. Create a new NIfTI image using the filtered data (preserving the original affine and header)\n",
    "    new_img = nib.Nifti1Image(data_filtered, affine=img.affine, header=img.header)\n",
    "    nib.save(new_img, output_nii_path)\n",
    "\n",
    "# Example usage:\n",
    "mask_file = \"/data484_2/xzhao14/FA_atlas/WM_ICBM_WMPM_1mm.nii\"\n",
    "#keep_values = [8, 48, 35]\n",
    "keep_values = [45, 1, 5]\n",
    "#keep_values = [5, 3, 35]\n",
    "output_file = \"/data484_2/xzhao14/FA_perd/fea_importance/MS.nii\"\n",
    "filter_mask(mask_file, keep_values, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "c3c91"
   },
   "outputs": [],
   "source": [
    "### Importance visualization ###\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def assign_values_to_mask(input_nii_path, value_map, output_nii_path):\n",
    "    \"\"\"\n",
    "    Reads a NIfTI mask file and assigns new values to specified voxel labels.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_nii_path : str\n",
    "        Path to the input NIfTI mask file (.nii or .nii.gz).\n",
    "    value_map : dict\n",
    "        Dictionary mapping original voxel values to new values, e.g., {45: 0.12, 1: 0.18, 5: 0.21}.\n",
    "    output_nii_path : str\n",
    "        Path to save the modified NIfTI file.\n",
    "    \"\"\"\n",
    "    # Load the input NIfTI file\n",
    "    img = nib.load(input_nii_path)\n",
    "    data = img.get_fdata()\n",
    "\n",
    "    # Create a new array with the same shape, initialized to 0\n",
    "    new_data = np.zeros_like(data)\n",
    "\n",
    "    # Assign new values to voxels based on the value_map\n",
    "    for original_value, new_value in value_map.items():\n",
    "        new_data[data == original_value] = new_value\n",
    "\n",
    "    # Save the modified image\n",
    "    new_img = nib.Nifti1Image(new_data, affine=img.affine, header=img.header)\n",
    "    nib.save(new_img, output_nii_path)\n",
    "\n",
    "# Example usage\n",
    "mask_file = \"/data484_2/xzhao14/FA_atlas/WM_ICBM_WMPM_1mm.nii\"\n",
    "for i in range(128):\n",
    "    UDIP_dimmension_x = pd.read_csv('/data484_2/xzhao14/FA_perd/sig_region_statistic/'+str(i)+'_sigre.csv')\n",
    "    value_map = dict(zip(UDIP_dimmension_x['ROI_value'].astype(int), UDIP_dimmension_x['observed_mean'].round(2)))\n",
    "    output_file = \"/data484_2/xzhao14/FA_perd/fea_importance/original_T_regions_WM/Dim_\"+str(i)+\".nii\"\n",
    "# keep_values = [8, 48, 35]\n",
    "# #keep_values = [45, 1, 5]\n",
    "# #keep_values = [5, 3, 35]\n",
    "# # Define the mapping from label values to new values\n",
    "# value_map = {\n",
    "#     8: 0.20,\n",
    "#     48:  0.13,\n",
    "#     35:  0.08\n",
    "# \n",
    "    assign_values_to_mask(mask_file, value_map, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "07b9b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "UDIP_dimmension_1 = pd.read_csv('/data484_2/xzhao14/FA_perd/sig_region_statistic/0_sigre.csv')\n",
    "value_map = dict(zip(UDIP_dimmension_1['ROI_value'].astype(int), UDIP_dimmension_1['observed_mean'].round(2)))\n",
    "print(value_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "06309"
   },
   "outputs": [],
   "source": [
    "UDIP_dimmension_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "8cf71"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_manhattan(df, chr_col='CHR', pos_col='POS', pval_col='P',\n",
    "                   genome_wide_line=5e-8, title='Manhattan Plot', figsize=(12, 6)):\n",
    "    \"\"\"\n",
    "    Plot a Manhattan plot for GWAS summary statistics.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing at least chromosome, position, and p-value columns.\n",
    "    chr_col : str\n",
    "        Column name for chromosomes.\n",
    "    pos_col : str\n",
    "        Column name for base-pair positions.\n",
    "    pval_col : str\n",
    "        Column name for p-values.\n",
    "    genome_wide_line : float\n",
    "        Genome-wide significance threshold (default = 5e-8).\n",
    "    title : str\n",
    "        Plot title.\n",
    "    figsize : tuple\n",
    "        Size of the plot (width, height).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Convert chromosomes to integers (handle X/Y if present)\n",
    "    df[chr_col] = df[chr_col].astype(str)\n",
    "    df[chr_col] = df[chr_col].str.replace('X', '23').str.replace('Y', '24')\n",
    "    df[chr_col] = df[chr_col].astype(int)\n",
    "\n",
    "    # Sort by chromosome and position\n",
    "    df = df.sort_values(by=[chr_col, pos_col])\n",
    "\n",
    "    # Add an index column for plotting on x-axis\n",
    "    df['ind'] = range(len(df))\n",
    "    df_grouped = df.groupby(chr_col)\n",
    "\n",
    "    # Initialize the plot\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    colors = ['#4daf4a', '#377eb8']  # Alternate green and blue\n",
    "    x_labels = []\n",
    "    x_labels_pos = []\n",
    "\n",
    "    # Loop through each chromosome and plot\n",
    "    for num, (chr_name, group) in enumerate(df_grouped):\n",
    "        group.plot(kind='scatter', x='ind', y=-np.log10(group[pval_col]),\n",
    "                   color=colors[num % len(colors)], ax=ax, s=10, alpha=0.6)\n",
    "        x_labels.append(chr_name)\n",
    "        x_labels_pos.append((group['ind'].iloc[-1] + group['ind'].iloc[0]) // 2)\n",
    "\n",
    "    # Add genome-wide significance line\n",
    "    ax.axhline(-np.log10(genome_wide_line), color='red', linestyle='--', linewidth=1)\n",
    "    ax.text(df['ind'].max()*0.98, -np.log10(genome_wide_line)+0.1,\n",
    "            f'p={genome_wide_line:.0e}', color='red', ha='right')\n",
    "\n",
    "    # Customize x-axis\n",
    "    ax.set_xticks(x_labels_pos)\n",
    "    ax.set_xticklabels(x_labels, rotation=0, fontsize=9)\n",
    "    ax.set_xlabel('Chromosome')\n",
    "    ax.set_ylabel('-log10(P-value)')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "e9cee"
   },
   "source": [
    "## Plot the classification result #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "3a4fc"
   },
   "outputs": [],
   "source": [
    "## \n",
    "# load pickle file\n",
    "disease_sample = '/data/xzhao14/UKB_sample_ICD10'\n",
    "all_disease_files = os.listdir(disease_sample)\n",
    "with open('/data/xzhao14/FA_classification_results1.pkl', \"rb\") as file:  # \"rb\" open in binary read mode\n",
    "    data = pickle.load(file)\n",
    "# print data\n",
    "data_list=data\n",
    "disorder_name=['PD','AD','DEP','MS','EPI','SCZ']\n",
    "selected_indices = [1,2,3,4,5,6] # indices to select ( 0 )\n",
    "data_list = [data_list[i] for i in selected_indices]\n",
    "auc_means = [df[\"AUC\"].mean() for df in data_list]\n",
    "auc_stds = [df[\"AUC\"].std() for df in data_list]\n",
    "print(np.mean(auc_means))\n",
    "print(np.std(auc_means))\n",
    "# plot bar chart\n",
    "x_labels = disorder_name# group labels\n",
    "x_positions = np.arange(len(data_list))  # x-axis positions\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x_positions, auc_means, yerr=auc_stds, capsize=5, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "\n",
    "# add title and axis labels\n",
    "#plt.title(\"Mean AUC with Standard Deviation\", fontsize=16)\n",
    "plt.xlabel(\"Brain disorders\", fontsize=14)\n",
    "plt.ylabel(\"AUC\", fontsize=14)\n",
    "plt.xticks(x_positions, x_labels, fontsize=12, rotation=45)\n",
    "# show figure\n",
    "plt.tight_layout()\n",
    "output_path = FA_figure_dir+\"/disorder_classification.jpg\"\n",
    "plt.savefig(output_path, format='jpg', dpi=300,bbox_inches='tight')\n",
    "print(f\"Plot saved as {output_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PerD Result Visualization\n",
    "\nVisualization of PerD results for different tissues (CSF, GM, WM) and different regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "313e0"
   },
   "outputs": [],
   "source": [
    "with open('/data/xzhao14/FA_classification_results1.pkl', \"rb\") as file:  # \"rb\" open in binary read mode\n",
    "    data = pickle.load(file)\n",
    "mean_all=[]\n",
    "for i in range(len(data)):\n",
    "    cc=data[i].describe()\n",
    "    print(cc)\n",
    "    #mean_all.append(cc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "e49ba"
   },
   "outputs": [],
   "source": [
    "## genetic correlation between different FA endophenotypes\n",
    "import os\n",
    "import subprocess\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def find_sumstats_files_in_subfolders(directory):\n",
    "    \"\"\"\n",
    "    Find all .fastGWA.sumstats.gz files in the subfolders of the given directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the root directory.\n",
    "\n",
    "    Returns:\n",
    "        list: List of file paths with .fastGWA.sumstats.gz extension.\n",
    "    \"\"\"\n",
    "    sumstats_files = []\n",
    "    # Iterate through the subdirectories\n",
    "    for subfolder in os.listdir(directory):\n",
    "        subfolder_path = os.path.join(directory, subfolder)\n",
    "        if os.path.isdir(subfolder_path):  # Check if it is a subdirectory\n",
    "            for file in os.listdir(subfolder_path):\n",
    "                if file.endswith(\".fastGWA.sumstats.gz\"):\n",
    "                    sumstats_files.append(os.path.join(subfolder_path, file))\n",
    "    return sumstats_files\n",
    "\n",
    "def calculate_genetic_correlation(file1, file2, ldsc_path, ld_ref_dir, out_dir):\n",
    "    \"\"\"\n",
    "    Calculate genetic correlation between two sumstats files using ldsc.\n",
    "\n",
    "    Args:\n",
    "        file1 (str): Path to the first .fastGWA.sumstats.gz file.\n",
    "        file2 (str): Path to the second .fastGWA.sumstats.gz file.\n",
    "        ldsc_path (str): Path to the ldsc.py script.\n",
    "        ld_ref_dir (str): Path to the LD reference directory.\n",
    "        out_dir (str): Path to the output directory.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the output file generated by ldsc.\n",
    "    \"\"\"\n",
    "    output_file = os.path.join(out_dir, f\"{os.path.basename(file1)}_vs_{os.path.basename(file2)}.rg\")\n",
    "    cmd = [\n",
    "        \"python\",\n",
    "        ldsc_path,\n",
    "        \"--rg\", file1 + \",\" + file2,\n",
    "        \"--ref-ld-chr\", os.path.join(ld_ref_dir, \"1000G_EUR_Phase3_baselineLD.\"),\n",
    "        \"--w-ld-chr\", os.path.join(ld_ref_dir, \"weights.\"),\n",
    "        \"--out\", output_file\n",
    "    ]\n",
    "    subprocess.call(cmd)\n",
    "    return output_file\n",
    "\n",
    "def process_file_pair(args):\n",
    "    \"\"\"\n",
    "    Wrapper for parallel processing of genetic correlation calculation.\n",
    "\n",
    "    Args:\n",
    "        args (tuple): Arguments for the calculate_genetic_correlation function.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the output file generated by ldsc.\n",
    "    \"\"\"\n",
    "    return calculate_genetic_correlation(*args)\n",
    "\n",
    "def main(directory, ldsc_path, ld_ref_dir, out_dir, num_processes=4):\n",
    "    \"\"\"\n",
    "    Main function to find sumstats files, calculate genetic correlations, and store results.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the root directory containing subfolders with sumstats files.\n",
    "        ldsc_path (str): Path to the ldsc.py script.\n",
    "        ld_ref_dir (str): Path to the LD reference directory.\n",
    "        out_dir (str): Path to the output directory.\n",
    "        num_processes (int): Number of parallel processes to use.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    sumstats_files = find_sumstats_files_in_subfolders(directory)\n",
    "\n",
    "    if not sumstats_files:\n",
    "        print(\"No .fastGWA.sumstats.gz files found in the subfolders.\")\n",
    "        return\n",
    "\n",
    "    # Prepare arguments for genetic correlation calculation\n",
    "    file_pairs = [\n",
    "        (file1, file2, ldsc_path, ld_ref_dir, out_dir)\n",
    "        for i, file1 in enumerate(sumstats_files)\n",
    "        for file2 in sumstats_files[i + 1:]\n",
    "    ]\n",
    "\n",
    "    # Use multiprocessing to calculate genetic correlations in parallel\n",
    "    pool = Pool(num_processes)\n",
    "    results = pool.map(process_file_pair, file_pairs)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    print(\"Genetic correlation calculation completed. Results:\")\n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    main(\n",
    "        directory=\"/data/xzhao14/GWAS_output\",  # Path to GWAS output directory\n",
    "        ldsc_path=\"/data484_2/xzhao14/POST_GWAS/tools/ldsc/ldsc.py\",  # Path to ldsc.py\n",
    "        ld_ref_dir=\"/data484_2/xzhao14/POST_GWAS/prepare_data/ldsc_files/eur_w_ld_chr/\",  # Path to LD reference directory\n",
    "        out_dir=\"output\",  # Path to store results\n",
    "        num_processes=10  # Number of parallel processes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "c5083"
   },
   "outputs": [],
   "source": [
    "## plot the result ##\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#root_dir = \"/data484_2/xzhao14/FA_meta/genetic_correlation_results/\"\n",
    "# \n",
    "root_dir='/data484_2/xzhao14/FA_meta/genetic_correlation_T1_FA_results'\n",
    "disorders = os.listdir(root_dir)\n",
    "data = []\n",
    "for i in disorders:\n",
    "    file_path = os.path.join(root_dir, i)\n",
    "    with open(file_path, 'r') as f:\n",
    "        all_lines = f.readlines()\n",
    "        if len(all_lines) < 4:\n",
    "            print(f\"Warning: File {file_path} does not have enough lines.\")\n",
    "            continue\n",
    "        last_line = all_lines[-4].strip()  # Extract the fourth last line\n",
    "        data.append(last_line.split())  # Split the line into individual values and store in the list\n",
    "\n",
    "# Define column names for the DataFrame\n",
    "columns = ['P1', 'P2', 'rg', 'SE', 'Z', 'P', 'h2_obs', 'h2_obs_se', 'h2_int', 'h2_int_se', 'gcov_int', 'gcov_int_se']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.replace(\"NA\", np.nan, inplace=True)\n",
    "df = df.dropna()\n",
    "print(len(df))\n",
    "# # # Convert 'rg' and 'SE' columns to float type\n",
    "# df['rg'] = df['rg'].astype(float)\n",
    "# df['SE'] = df['SE'].astype(float)\n",
    "# df['P1'] = df['P1'].apply(lambda x: x.split('/')[-1].split('_')[0])\n",
    "# df['P2'] = df['P2'].apply(lambda x: x.split('/')[-1].split('_')[0])\n",
    "# df['P2'] = df['P2'].str.replace(r'^QT', 'EP', regex=True)\n",
    "# # sort P2 by QT0–QT127\n",
    "# df['P2'] = pd.Categorical(df['P2'], categories=[f\"EP{i}\" for i in range(128)], ordered=True)\n",
    "# df = df.sort_values(by='P2') # sort by P2\n",
    "# df.loc[(df['rg'] >= 1.2) & (df['rg'] <= 1.3), 'rg'] = 1.1\n",
    "# df.loc[(df['rg'] >= 1.3) & (df['rg'] <= 1.4), 'rg'] = 1.2\n",
    "# df.loc[df['rg'] > 1.5, 'rg'] = 1.3\n",
    "# print(df['rg'].mean())\n",
    "# print(df['rg'].std())\n",
    "# df[\"FDR\"] = multipletests(df[\"P\"].astype(float), alpha=0.05, method='fdr_bh')[1]\n",
    "# colors = ['orange' if fdr < 0.05 else 'blue' for fdr in df['FDR']]\n",
    "# print(df['FDR'].mean())\n",
    "# print(df['FDR'].std())\n",
    "\n",
    "# # Generate the forest plot\n",
    "# plt.figure(figsize=(10, 20))\n",
    "\n",
    "# # Draw scatter points with error bars\n",
    "# for i in range(len(df)):\n",
    "#     plt.errorbar(\n",
    "#         df['rg'].iloc[i], df['P2'].iloc[i],\n",
    "#         xerr=df['SE'].iloc[i], fmt='o', \n",
    "#         color=colors[i], ecolor='black', capsize=3\n",
    "#     )\n",
    "\n",
    "# # Add legend\n",
    "# handles = [\n",
    "#     plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='orange', markersize=8, label='FDR < 0.05'),\n",
    "#     plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=8, label='FDR ≥ 0.05')\n",
    "# ]\n",
    "# plt.legend(handles=handles, title='Significance', loc='upper left', fontsize=10)\n",
    "\n",
    "# # Add decorations\n",
    "# plt.axvline(x=0, color='gray', linestyle='--', linewidth=1)  # Add a vertical line at x=0\n",
    "# plt.xlabel('Genetic Correlation (rg)', fontsize=12)\n",
    "# plt.ylabel('Endophenotypes', fontsize=12)\n",
    "# plt.title('Genetic Correlation of discovery and replication', fontsize=14)\n",
    "# plt.gca().invert_yaxis()  # Reverse Y-axis to align with table layout\n",
    "# plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "# output_path = FA_figure_dir+\"/genetic_correlation_discover_replications.pdf\"\n",
    "# plt.savefig(output_path, format='pdf', bbox_inches='tight')\n",
    "# print(f\"Plot saved as {output_path}\")\n",
    "# # Display the plot\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "f2122"
   },
   "outputs": [],
   "source": [
    "print(np.abs(df['rg']).mean())\n",
    "print(np.abs(df['rg']).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "52cff"
   },
   "source": [
    "## comparison of heritability between our and previous "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "d8095"
   },
   "source": [
    "# get the significant SNP using the minP strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "f55ba"
   },
   "outputs": [],
   "source": [
    "## using the single variants GWAS result to get the significant result ##\n",
    "from pathlib import Path\n",
    "import os, numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from subprocess import check_output, STDOUT\n",
    "from itertools import zip_longest\n",
    "from glob import glob\n",
    "def extract(x, exclusion, out_path):\n",
    "    # extract rows in the file x not contained in exclusion and save in out_path.\n",
    "    x = Path(x)\n",
    "    out_path = Path(out_path)\n",
    "    cmd = f\"awk 'NR == FNR {{ excl[$1]; next }} !(FNR in excl)' {exclusion} {x} > {x.parent/out_path/(x.name.split('.')[0] + '_extracted')}\"\n",
    "    os.system(cmd)\n",
    "    \n",
    "def extract_col(x, offset=0):\n",
    "    out = check_output(f\"awk '{{print $(NF-{offset})}}' {x}\", universal_newlines=True, shell=True, stderr=STDOUT)\n",
    "    return np.array(list(map(float, out.strip('\\n').split('\\n')[1:]))), x\n",
    "\n",
    "def create_minP(glob_list, pcol=0, mode='min'):\n",
    "    # pcol is indexed from right to left, 0 means last col\n",
    "    if mode == 'min':\n",
    "        op = np.argmin\n",
    "    elif mode == 'max':\n",
    "        op = np.argmax\n",
    "    else:\n",
    "        raise Exception('not implemented')\n",
    "    batch = 50\n",
    "    for i in tqdm(range(0, len(glob_list), batch)):\n",
    "        with Pool(batch) as q:\n",
    "            result = q.starmap(extract_col, zip_longest(glob_list[i:i+batch], (), fillvalue=pcol))\n",
    "        pnew, fnew = list(zip(*result))\n",
    "        if i == 0:\n",
    "            pnew = np.vstack(pnew)\n",
    "            idx = op(pnew, 0)\n",
    "            f = np.array(fnew)[idx]\n",
    "        else:\n",
    "            pnew = list(pnew)\n",
    "            pnew.append(p)\n",
    "            pnew = np.vstack(pnew)\n",
    "            idx = op(pnew, 0)\n",
    "            mask = (idx != (pnew.shape[0]-1))\n",
    "            f[mask] = np.array(fnew)[idx[mask]]\n",
    "        p = pnew[idx, np.arange(pnew.shape[1])]\n",
    "    return p, f\n",
    "## Get the minP  ##\n",
    "p, f = create_minP(glob(\"/data484_2/xzhao14/FA_meta/QT*_FA.txt\"), 0)\n",
    "####   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "174f2"
   },
   "outputs": [],
   "source": [
    "### built  the  GWAS file for the meta single variant gWAS result ###\n",
    "# Prepare a GWAS summary statisitc file and updata the p value \n",
    "import pandas as pd\n",
    "single_variant_gwas=pd.read_csv('/data484_2/xzhao14/FA_meta/QT97_FA.txt',sep='\\t')\n",
    "single_variant_gwas['P']=p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "ea583"
   },
   "outputs": [],
   "source": [
    "single_variant_gwas.to_csv('/data/xzhao14/FA_meta.txt',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "4832a"
   },
   "outputs": [],
   "source": [
    "## previous Heritability \n",
    "import os\n",
    "from glob import glob\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_heritability_and_pval(file_path):\n",
    "    \"\"\"\n",
    "    Extract heritability and p-value from a given file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Define the regular expression to match heritability and p-value\n",
    "    heritability_pattern = r\"Heritability\\s*=\\s*([\\d\\.]+)\\s*\\(Pval\\s*=\\s*([\\d\\.eE\\-]+)\\)\"\n",
    "    \n",
    "    # Search for the pattern in the content\n",
    "    match = re.search(heritability_pattern, content)\n",
    "    if match:\n",
    "        heritability = float(match.group(1))  # Extract the heritability value\n",
    "        p_value = float(match.group(2))      # Extract the p-value\n",
    "        return heritability, p_value\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def process_directory(directory):\n",
    "    \"\"\"\n",
    "    Process all files ending with 'results.log' in the given directory.\n",
    "    Extract heritability and p-value, and return a DataFrame.\n",
    "    \"\"\"\n",
    "    # Prepare a list to store the results\n",
    "    results = []\n",
    "    # Walk through the directory to find all matching files\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".log\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                heritability, p_value = extract_heritability_and_pval(file_path)\n",
    "                results.append({\n",
    "                    'File': file,\n",
    "                    'Heritability': heritability,\n",
    "                    'P-value': p_value\n",
    "                })\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "def get_gc_file(disorders,root_dir):\n",
    "    data = []\n",
    "    for i in disorders:\n",
    "        file_path = os.path.join(root_dir, i)\n",
    "        with open(file_path, 'r') as f:\n",
    "            all_lines = f.readlines()\n",
    "            if len(all_lines) < 4:\n",
    "                print(f\"Warning: File {file_path} does not have enough lines.\")\n",
    "                continue\n",
    "            last_line = all_lines[-4].strip()  # Extract the fourth last line\n",
    "            data.append(last_line.split())  # Split the line into individual values and store in the list\n",
    "\n",
    "    # Define column names for the DataFrame\n",
    "    columns = ['P1', 'P2', 'rg', 'SE', 'Z', 'P', 'h2_obs', 'h2_obs_se', 'h2_int', 'h2_int_se', 'gcov_int', 'gcov_int_se']\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    df.replace(\"NA\", np.nan, inplace=True)\n",
    "    df = df.dropna()\n",
    "    # Convert 'rg' and 'SE' columns to float type\n",
    "    df['rg'] = df['rg'].astype(float)\n",
    "    df['SE'] = df['SE'].astype(float)\n",
    "    df['h2_obs'] = df['h2_obs'].astype(float)\n",
    "    return df\n",
    "def get_file_list(directory, pattern):\n",
    "    \"\"\"\n",
    "    Get a list of files matching the pattern from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The directory to search.\n",
    "        pattern (str): The file name pattern to match.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of file paths matching the pattern.\n",
    "    \"\"\"\n",
    "    # Construct the full search path\n",
    "    search_path = os.path.join(directory, pattern)\n",
    "    \n",
    "    # Use glob to find all matching files\n",
    "    file_list = glob(search_path)\n",
    "    \n",
    "    return file_list\n",
    "\n",
    "# # Example usage\n",
    "root_dir = \"/data/xzhao14/FA_previous_gc/\"  # Replace with your directory path\n",
    "pattern = \"QT127_results_ukb_phase1to3_fapcs_dec21_2019_pheno*_rg_results.log\"\n",
    "\n",
    "# # Get the file list\n",
    "files = get_file_list(root_dir, pattern)\n",
    "previous_gc=get_gc_file(files,root_dir)\n",
    "# previous_h2_files=pd.read_csv('/data/xzhao14/fa_pc_h2.txt')\n",
    "# previous_h2=previous_h2_files['h2']\n",
    "# # get our now result #\n",
    "# #h2_directory = \"/data484_2/xzhao14/FA_statis\"  # Replace with your directory path\n",
    "# h2_directory = \"//data484_2/xzhao14/FA_fea_all/GWAS\"  # Replace with your directory path#\n",
    "# h2_fa = process_directory(h2_directory)\n",
    "# our_h2=h2_fa['Heritability']\n",
    "# h2_fa[\"FDR\"] = multipletests(h2_fa[\"P-value\"].astype(float), alpha=0.05, method='fdr_bh')[1]\n",
    "# ### plot the box plot #\n",
    "# # \"_results.log\"\n",
    "# h2_fa['File'] = h2_fa['File'].str.replace(\"_results.log\", \"\", regex=False)\n",
    "\n",
    "# # \"QT\" \"EP\"\n",
    "# h2_fa['File'] = h2_fa['File'].str.replace(\"QT\", \"UDIP\", regex=False)\n",
    "# h2_fa['File_number'] = h2_fa['File'].str.extract(r'UDIP(\\d+)', expand=False).astype(int) \n",
    "# h2_fa = h2_fa.sort_values(by='File_number').drop(columns=['File_number'])  \n",
    "# #h2_fa.to_csv(FA_figure_dir+'/FA_h2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "6e19d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "FA_heritability_all=pd.read_csv('/data/xzhao14/FA_figures/FA_heritability.csv')\n",
    "heri_value=FA_heritability_all['Heritability'].values\n",
    "np.save('/data484_2/xzhao14/heritability.npy', heri_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "e985b"
   },
   "outputs": [],
   "source": [
    "dd=np.load('/data484_2/xzhao14/heritability.npy')\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "ae4e7"
   },
   "outputs": [],
   "source": [
    "np.mean(h2_fa['Heritability'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "df277"
   },
   "outputs": [],
   "source": [
    "# Plot the heritability distribution\n",
    "# Set figure size and style\n",
    "plt.figure(figsize=(10, 6))\n",
    "heritability_values = h2_fa['Heritability']\n",
    "\n",
    "# Draw a histogram of heritability values\n",
    "plt.hist(heritability_values, bins=20, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Compute mean heritability and define a threshold for \"high heritability\"\n",
    "mean_heritability = np.mean(heritability_values)\n",
    "threshold = 0.6  # threshold to mark high heritability\n",
    "\n",
    "# Add vertical lines for the mean and the threshold\n",
    "plt.axvline(mean_heritability, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Mean: {mean_heritability:.2f}')\n",
    "plt.axvline(threshold, color='green', linestyle='--', linewidth=2,\n",
    "            label=f'Threshold: {threshold}')\n",
    "\n",
    "# Set axis labels (and optionally a title)\n",
    "# plt.title('Distribution of Heritability Across Phenotypes', fontsize=16)\n",
    "plt.xlabel('SNP Heritability', fontsize=16)\n",
    "plt.ylabel('Frequency', fontsize=16)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "# Annotate the number of high heritability values\n",
    "high_heritability_count = sum(heritability_values > threshold)\n",
    "plt.text(threshold + 0.02, 5,\n",
    "         f'High heritability: {high_heritability_count}',\n",
    "         color='green', fontsize=12)\n",
    "\n",
    "# Improve layout and tick label sizes\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure as a PDF\n",
    "output_path = FA_figure_dir + \"/h2_FA.pdf\"\n",
    "plt.savefig(output_path, format='pdf', bbox_inches='tight')\n",
    "print(f\"Plot saved as {output_path}\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "ed6b8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Read the first CSV file and compute the upper triangle of the correlation matrix\n",
    "file1 = '/data/xzhao14/discovery_dti_128_pheno.csv'  # Replace with actual file path\n",
    "df1 = pd.read_csv(file1)\n",
    "\n",
    "# Extract columns QT0 to QT127\n",
    "qt_columns = [f'QT{i}' for i in range(128)]\n",
    "qt_data = df1[qt_columns]\n",
    "\n",
    "# Compute the correlation matrix among the QT features\n",
    "correlation_matrix = qt_data.corr()\n",
    "\n",
    "# Extract the upper triangular part (excluding the diagonal)\n",
    "upper_triangle = np.triu(correlation_matrix, k=1)\n",
    "\n",
    "# 2. Read the second CSV file and fill in the lower triangle with genetic correlations\n",
    "file2 = '/data/xzhao14/genetic_cor.csv'  # Replace with actual file path\n",
    "df2 = pd.read_csv(file2)\n",
    "\n",
    "# Initialize a 128×128 zero matrix for the lower triangle\n",
    "lower_triangle = np.zeros_like(correlation_matrix, dtype=float)\n",
    "\n",
    "# Populate the lower triangle entries with 'rg' values from df2\n",
    "for _, row in df2.iterrows():\n",
    "    qt1_name = row['P1']  # e.g. 'QT0'\n",
    "    qt2_name = row['P2']  # e.g. 'QT1'\n",
    "    if qt1_name.startswith('QT') and qt2_name.startswith('QT'):\n",
    "        try:\n",
    "            qt1_idx = int(qt1_name[2:])  # Convert 'QT#' to numeric index\n",
    "            qt2_idx = int(qt2_name[2:])\n",
    "            lower_triangle[qt1_idx, qt2_idx] = row['rg']\n",
    "            lower_triangle[qt2_idx, qt1_idx] = row['rg']  # Symmetric fill\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping invalid QT pair: {qt1_name}, {qt2_name}. Error: {e}\")\n",
    "    else:\n",
    "        print(f\"Skipping invalid QT names: {qt1_name}, {qt2_name}\")\n",
    "\n",
    "# Retain only the lower triangle (below the diagonal)\n",
    "lower_triangle = np.tril(lower_triangle)\n",
    "\n",
    "# 3. Extract non-zero upper-triangle values and standardize them\n",
    "upper_triangle_values = upper_triangle[upper_triangle != 0]\n",
    "scaler = StandardScaler()\n",
    "upper_standardized = scaler.fit_transform(upper_triangle_values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Rebuild the standardized upper-triangle matrix\n",
    "upper_triangle_normalized = np.zeros_like(correlation_matrix)\n",
    "upper_triangle_normalized[np.triu_indices_from(upper_triangle, k=1)] = upper_standardized\n",
    "\n",
    "# 4. Extract non-zero lower-triangle values and standardize them\n",
    "lower_triangle_values = lower_triangle[lower_triangle != 0]\n",
    "lower_standardized = scaler.fit_transform(lower_triangle_values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Rebuild the standardized lower-triangle matrix\n",
    "lower_triangle_normalized = np.zeros_like(correlation_matrix)\n",
    "lower_triangle_normalized[np.tril_indices_from(lower_triangle, k=-1)] = lower_standardized\n",
    "\n",
    "# 5. Combine raw upper and lower triangles into a full matrix\n",
    "full_matrix = upper_triangle + lower_triangle\n",
    "\n",
    "# 6. (Optional) Combine the standardized triangles into a normalized full matrix\n",
    "full_matrix_normalized = upper_triangle_normalized + lower_triangle_normalized\n",
    "\n",
    "# Visualize the absolute correlation heatmap\n",
    "full_matrix_abs = np.abs(full_matrix)\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(full_matrix_abs, cmap='coolwarm', annot=False, fmt='.2f')\n",
    "\n",
    "# Save the heatmap to a PDF\n",
    "output_path = FA_figure_dir + \"/genome_pheotype_correlatio.pdf\"\n",
    "plt.savefig(output_path, format='pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "134fe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# Example data: absolute values of upper- and lower-triangle correlation matrices\n",
    "pheno = np.abs(upper_triangle_values)\n",
    "genome = np.abs(lower_triangle_values)\n",
    "\n",
    "# Print group means for reference\n",
    "print(\"Mean Pheno correlation:\", np.mean(pheno))\n",
    "print(\"Mean Genome correlation:\", np.mean(genome))\n",
    "\n",
    "# Perform the Wilcoxon signed-rank test for paired samples\n",
    "stat, p_value = wilcoxon(pheno, genome)\n",
    "\n",
    "# Prepare a DataFrame for plotting\n",
    "data = pd.DataFrame({\n",
    "    'Correlation': np.concatenate([pheno, genome]),\n",
    "    'Group': ['Pheno'] * len(pheno) + ['Genome'] * len(genome)\n",
    "})\n",
    "\n",
    "# Create a boxplot comparing the two groups\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='Group', y='Correlation', data=data, palette=\"Set2\")\n",
    "plt.title('Boxplot of Pheno vs. Genome Correlations with Wilcoxon Test', fontsize=16)\n",
    "\n",
    "# Annotate the plot with the Wilcoxon p-value\n",
    "plt.text(\n",
    "    0.5, \n",
    "    max(data['Correlation']) + 0.2, \n",
    "    f'Wilcoxon p-value: {p_value:.3e}', \n",
    "    ha='center', \n",
    "    fontsize=12, \n",
    "    color='red'\n",
    ")\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "d572e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Create a custom color map: blue for negative values, white at zero, red for positive values\n",
    "cmap = LinearSegmentedColormap.from_list(\n",
    "    'custom_cmap', ['blue', 'white', 'red'], N=100\n",
    ")\n",
    "\n",
    "# Ensure the matrix values are in the range [-1, 1]; take absolute values if needed\n",
    "# full_matrix_normalized = np.abs(full_matrix_normalized)\n",
    "\n",
    "# Plot the clustermap\n",
    "plt.figure(figsize=(20, 12))\n",
    "clustermap = sns.clustermap(\n",
    "    full_matrix,               # Input correlation matrix\n",
    "    cmap=cmap,                 # Use the custom color map\n",
    "    annot=False,               # Disable cell annotations\n",
    "    fmt='.2f',                 # Format for annotation if enabled\n",
    "    xticklabels=True,          # Show x-axis tick labels\n",
    "    yticklabels=True,          # Show y-axis tick labels\n",
    "    vmin=-1, vmax=1,           # Set the value range for the color scale\n",
    "    figsize=(12, 10),          # Figure size for the clustermap\n",
    "    annot_kws={\"size\": 6},     # Font size for annotations if enabled\n",
    "    row_cluster=False,         # Disable row clustering\n",
    "    col_cluster=False,         # Disable column clustering\n",
    "    cbar_pos=(0.02, 0.8, 0.03, 0.18),  # Position of the colorbar (x, y, width, height)\n",
    "    cbar_kws={\n",
    "        \"label\": \"Correlation\",    # Label for the colorbar\n",
    "        \"shrink\": 0.5,             # Shrink factor for the colorbar\n",
    "        \"orientation\": \"vertical\",\n",
    "        \"format\": \"%.2f\"           # Format for colorbar tick labels\n",
    "    },\n",
    ")\n",
    "\n",
    "# Adjust the colorbar tick label font size\n",
    "colorbar = clustermap.ax_heatmap.collections[0].colorbar\n",
    "colorbar.ax.tick_params(labelsize=8)\n",
    "\n",
    "# Set the colorbar label font size\n",
    "colorbar.set_label(\"Correlation\", fontsize=16)\n",
    "\n",
    "# Adjust the axis tick label font sizes\n",
    "plt.setp(clustermap.ax_heatmap.xaxis.get_majorticklabels(), fontsize=12)\n",
    "plt.setp(clustermap.ax_heatmap.yaxis.get_majorticklabels(), fontsize=12)\n",
    "\n",
    "# Optionally set a title for the plot\n",
    "# plt.title('Clustermap of Correlation Matrix', fontsize=16)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "d504a"
   },
   "outputs": [],
   "source": [
    "# Par Her\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def extract_data_from_parh_subfolders(folder_path):\n",
    "    \"\"\"\n",
    "    Extracts the first row of data from '.results' files in the 'parh' folder within each subfolder,\n",
    "    combines them into separate DataFrames for each subfolder, and computes the mean\n",
    "    across all combined DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): Path to the parent folder containing subfolders with 'parh' folders.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the mean values of all combined DataFrames.\n",
    "    \"\"\"\n",
    "    all_parh_dfs = []  # List to store combined DataFrames from each 'parh' folder\n",
    "\n",
    "    # Iterate through each subfolder in the parent folder\n",
    "    for subfolder_name in os.listdir(folder_path):\n",
    "        subfolder_path = os.path.join(folder_path, subfolder_name)\n",
    "      \n",
    "        parh_folder_path = os.path.join(subfolder_path, 'parh')  # Path to 'parh' folder\n",
    "        #print(parh_folder_path)\n",
    "        if os.path.isdir(parh_folder_path):  # Ensure the 'parh' folder exists\n",
    "           # print('sss')\n",
    "            parh_data = []  # List to store data from files in 'parh' folder\n",
    "\n",
    "            # Process each '.results' file in the 'parh' folder\n",
    "            for file_name in os.listdir(parh_folder_path):\n",
    "                if file_name.endswith(\".results\"):  # Process only '.results' files\n",
    "                    file_path = os.path.join(parh_folder_path, file_name)\n",
    "                    try:\n",
    "                        # Read the file\n",
    "                        df = pd.read_csv(file_path, sep=\"\\s+\", engine=\"python\")\n",
    "                        \n",
    "                        if len(df) > 0:  # Ensure the file contains data\n",
    "                            # Extract the first row\n",
    "                            first_row = df.iloc[0].to_frame().T\n",
    "                            first_row['File_Identifier'] = file_name.split('.result')[0].split('_')[2]  # Extract identifier\n",
    "                            parh_data.append(first_row)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_name} in {parh_folder_path}: {e}\")\n",
    "\n",
    "            # Combine all rows from this 'parh' folder into a single DataFrame\n",
    "            if parh_data:\n",
    "                print(parh_data)\n",
    "                parh_df = pd.concat(parh_data, ignore_index=True)\n",
    "                all_parh_dfs.append(parh_df)\n",
    "\n",
    "    # Aggregate data: sum all 'parh' DataFrames and compute mean\n",
    "    if all_parh_dfs:\n",
    "        combined_df = pd.concat(all_parh_dfs, ignore_index=True)  # Combine all 'parh' DataFrames\n",
    "        aggregated_df = combined_df.mean(numeric_only=True)  # Compute mean of numeric columns\n",
    "        return aggregated_df\n",
    "    else:\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no data is found\n",
    "\n",
    "# Example usag\n",
    "# Example usage\n",
    "parent_folder = \"/data/xzhao14/GWAS_output\"  # Replace with the path to your parent folder\n",
    "result_df = extract_data_from_parh_subfolders(parent_folder)\n",
    "\n",
    "# Print or save the result\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "54feb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "dd=pd.read_csv(os.path.join('/data484_2/xzhao14/', \"filtered_combined_genes.csv\"))\n",
    "cc_dd=list(dd['GENE'])\n",
    "gene_symbol=pd.read_csv('/data484_2/xzhao14/POST_GWAS/prepare_data/MAGMA_files/NCBI37.3.gene.loc',sep='\\t',header=None)\n",
    "gene_symbol.columns = ['ID', 'CHR', 'START','END','Dir','Gene_system']\n",
    "filtered_df = gene_symbol[gene_symbol['ID'].isin(cc_dd)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "e0f43"
   },
   "outputs": [],
   "source": [
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "f1960"
   },
   "outputs": [],
   "source": [
    "combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "e408a"
   },
   "outputs": [],
   "source": [
    "## previous published data #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.colors import to_hex, Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from collections import Counter\n",
    "science_data=pd.read_csv('/data/xzhao14/previous_res.txt')\n",
    "this_result=pd.read_csv('/data484_2/xzhao14/FA_FUMA_job577004/FA_meta_5e-8_lead_SNP.txt',sep='\\t')\n",
    "### ###\n",
    "SNP_list=list(this_result['rsID'])\n",
    "\n",
    "def filter_gwas_catalog(snp_list, gwas_catalog_file, output_file):\n",
    "    \"\"\"\n",
    "    Filters rows containing specific SNPs from the GWAS Catalog file and retains selected columns.\n",
    "\n",
    "    Parameters:\n",
    "        snp_list_file (str): Path to the file containing the list of SNPs (one SNP per line).\n",
    "        gwas_catalog_file (str): Path to the GWAS Catalog file (tab-separated).\n",
    "        output_file (str): Path to save the filtered results.\n",
    "    \"\"\"\n",
    "    # Columns to retain\n",
    "    columns_to_keep = [\n",
    "        'PUBMEDID', 'FIRST AUTHOR', 'JOURNAL', 'DISEASE/TRAIT', \n",
    "        'INITIAL SAMPLE SIZE', 'CHR_ID', 'CHR_POS', \n",
    "        'REPORTED GENE(S)', 'MAPPED_GENE', 'SNPS', \n",
    "        'RISK ALLELE FREQUENCY', 'P-VALUE', 'PVALUE_MLOG', \n",
    "        'P-VALUE (TEXT)', 'OR or BETA'\n",
    "    ]\n",
    "    # Read the SNP list\n",
    "    # Read the GWAS Catalog file\n",
    "    gwas_df = pd.read_csv(gwas_catalog_file, sep='\\t')\n",
    "\n",
    "    # Filter rows containing specified SNPs\n",
    "    filtered_df = gwas_df[gwas_df['SNPS'].isin(snp_list)]\n",
    "\n",
    "    # Retain only the specified columns\n",
    "    filtered_df = filtered_df[columns_to_keep]\n",
    "    print(f\"Filtering complete! Results saved to {output_file}\")\n",
    "  \n",
    "    # Save the filtered results to the output file\n",
    "    filtered_df.to_csv(output_file, sep='\\t', index=False)\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "gwas_catalog_file = '/data484_2/xzhao14/POST_GWAS/prepare_data/GWAS_catelog/GWAS_catelog.txt'  # Path to the GWAS Catalog file\n",
    "output_file = '/data484_2/xzhao14/FA_cate_log_meta_5e-8_filtered_gwas_catalog.txt'  # Path to save the results\n",
    "\n",
    "GWAS_asso=filter_gwas_catalog(SNP_list,gwas_catalog_file,output_file)\n",
    "df=GWAS_asso[['DISEASE/TRAIT','P-VALUE']]\n",
    "# Example input: a dataframe with two columns: 'Text' and 'P_Value'\n",
    "# Calculate word frequencies\n",
    "word_counts = Counter(df['DISEASE/TRAIT'])\n",
    "\n",
    "# Find the minimum P-Value for each word\n",
    "word_min_p_values = df.groupby('DISEASE/TRAIT')['P-VALUE'].min()\n",
    "\n",
    "# Normalize P-Values for color mapping\n",
    "norm = Normalize(vmin=word_min_p_values.min(), vmax=word_min_p_values.max())\n",
    "\n",
    "# Custom color function for shades of red based on P-values\n",
    "def word_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    if word in word_min_p_values:\n",
    "        normalized_value = norm(word_min_p_values[word])  # Normalize the P-value\n",
    "        red_intensity = int(255 * (1 - normalized_value))  # Invert to make smaller P-values darker\n",
    "        return f\"rgb({red_intensity}, 0, 0)\"\n",
    "    else:\n",
    "        return \"rgb(255, 255, 255)\"  # Default to white if word not found (unlikely)\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color=\"white\",\n",
    "    color_func=word_color_func\n",
    ").generate_from_frequencies(word_counts)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Association with other phenotype using SNP\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "a2e68"
   },
   "source": [
    "#### replication analysis  ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "98544"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "science_data=pd.read_csv('/data/xzhao14/previous_res.txt')\n",
    "this_result=pd.read_csv('/data484_2/xzhao14/FA_FUMA_job577004/leadSNPs.txt',sep='\\t')\n",
    "#this_result=pd.read_csv('/data/xzhao14/FA_clump.csv')\n",
    "GWAS_catelog=pd.read_csv('/data484_2/xzhao14/FA_cate_log_minp_filtered_gwas_catalog.txt',sep='\\t')\n",
    "GWAS_catelog = GWAS_catelog[GWAS_catelog['DISEASE/TRAIT'].str.contains(r\"brain|white|cortical\", case=False, na=False)]\n",
    "replication_res=pd.read_csv('/data484_2/xzhao14/FA_replication5_8_sig_snp.txt',sep='\\t')\n",
    "print(len(set(GWAS_catelog['SNPS'])&set(this_result['rsID'])))\n",
    "print(len(set(science_data['SNP_science'])&set(this_result['rsID'])))\n",
    "print(len(set(replication_res['SNP'])&set(this_result['rsID'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "d35c0"
   },
   "outputs": [],
   "source": [
    "this_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "8527d"
   },
   "outputs": [],
   "source": [
    "# replication \n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import pandas  as pd\n",
    "root_dir='/data/xzhao14/FA_rep_gc'\n",
    "disorders = os.listdir(root_dir)\n",
    "data = []\n",
    "for i in disorders:\n",
    "    file_path = os.path.join(root_dir, i)\n",
    "    with open(file_path, 'r') as f:\n",
    "        all_lines = f.readlines()\n",
    "        if len(all_lines) < 4:\n",
    "            print(f\"Warning: File {file_path} does not have enough lines.\")\n",
    "            continue\n",
    "        last_line = all_lines[-4].strip()  # Extract the fourth last line\n",
    "        data.append(last_line.split())  # Split the line into individual values and store in the list\n",
    "\n",
    "# Define column names for the DataFrame\n",
    "columns = ['P1', 'P2', 'rg', 'SE', 'Z', 'P', 'h2_obs', 'h2_obs_se', 'h2_int', 'h2_int_se', 'gcov_int', 'gcov_int_se']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df['P1'] = df['P1'].str.extract(r'/([^/]+)\\.sumstats\\.gz')[0]\n",
    "df['P2'] = df['P2'].str.extract(r'/([^/]+)\\.sumstats\\.gz')[0]\n",
    "df.replace(\"NA\", np.nan, inplace=True)\n",
    "df = df.dropna()\n",
    "print(df)\n",
    "# Convert 'rg' and 'SE' columns to float type\n",
    "df['rg'] = df['rg'].astype(float)\n",
    "df['SE'] = df['SE'].astype(float)\n",
    "df[\"FDR\"]=multipletests(df[\"P\"].astype(float),alpha=0.05, method='fdr_bh')[1]\n",
    "    # Determine color based on FDR value\n",
    "  # Determine color based on FDR value\n",
    "colors = ['orange' if fdr < 0.05 else 'blue' for fdr in df['FDR']]\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Draw scatter points with error bars\n",
    "for i in range(len(df)):\n",
    "    plt.errorbar(\n",
    "        df['rg'].iloc[i], df['P2'].iloc[i],\n",
    "        xerr=df['SE'].iloc[i], fmt='o', \n",
    "        color=colors[i], ecolor='black', capsize=3\n",
    "    )\n",
    "\n",
    "    # Add legend\n",
    "    handles = [\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='orange', markersize=8, label='FDR < 0.05'),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=8, label='FDR ≥ 0.05')\n",
    "    ]\n",
    "    plt.legend(handles=handles, title='Significance', loc='upper left', fontsize=10)\n",
    "\n",
    "    # Add decorations\n",
    "    plt.axvline(x=0, color='gray', linestyle='--', linewidth=1)  # Add a vertical line at x=0\n",
    "    plt.xlabel('Genetic Correlation (rg)', fontsize=12)\n",
    "    plt.ylabel('P2 Phenotypes', fontsize=12)\n",
    "    plt.title('Forest Plot of Genetic Correlation', fontsize=14)\n",
    "    plt.gca().invert_yaxis()  # Reverse Y-axis to align with table layout\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "89336"
   },
   "outputs": [],
   "source": [
    "# partioned  heritability analysis  #\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "def extract_first_row_with_pandas(folder_path,index):\n",
    "    \"\"\"\n",
    "    Extracts the first row of data from each '.results' file in the given folder,\n",
    "    combines them into a single DataFrame, and adds an identifier column extracted from the file name.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): Path to the folder containing '.results' files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with combined data from all files and a file identifier column.\n",
    "    \"\"\"\n",
    "    all_data = []  # List to store DataFrames from each file\n",
    "   # print(os.listdir(folder_path))\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        #print(file_name)\n",
    "        if file_name.endswith(\".results\"):  # Process only files with '.results' suffix\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            try:\n",
    "                #print(file_path)\n",
    "                # Read the file, automatically using the first row as column names\n",
    "                df = pd.read_csv(file_path,sep='\\t')\n",
    "               \n",
    "                if len(df) > 0:  # Ensure the file contains data\n",
    "                    # Extract the first row as a DataFrame\n",
    "                    first_row = df.iloc[0].to_frame().T\n",
    "                    first_row['File_Identifier'] = file_name.split('.result')[0].split('_')[2] # Extract identifier from file name\n",
    "                    all_data.append(first_row)  # Add the first row to the list\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_name}: {e}\")\n",
    "    #print(combined_df)\n",
    "    # Combine all rows into a single DataFrame\n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    else:\n",
    "        combined_df = pd.DataFrame()  # Return an empty DataFrame if no data is found\n",
    "    combined_df['pheno']=index\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def plot_bubble_chart_vertical(df, p_col='Enrichment_p', identifier_col='File_Identifier', alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform FDR correction on a specified p-value column, transform to -log10 scale, and plot a vertical bubble chart.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame containing p-values and identifiers.\n",
    "        p_col (str): Column name containing p-values.\n",
    "        identifier_col (str): Column name containing file identifiers.\n",
    "        alpha (float): Significance level for FDR correction.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the bubble chart.\n",
    "    \"\"\"\n",
    "    # Perform FDR correction\n",
    "    file_path = \"/data/xzhao14/Supplemental_tables.csv\"\n",
    "    p_values = df[p_col].astype(float)  # Ensure p-values are in float format\n",
    "    corrected = multipletests(p_values, alpha=alpha, method='fdr_bh')  # FDR correction\n",
    "    df['FDR_Adjusted_p'] = corrected[1]  # Store adjusted p-values\n",
    "\n",
    "    # Transform to -log10 scale\n",
    "    df['-log10(FDR_Adjusted_p)'] = -np.log10(df['FDR_Adjusted_p'])\n",
    "    df = df.dropna()\n",
    "    if (df['-log10(FDR_Adjusted_p)']<1.3).any():\n",
    "    #plot_bubble_chart_vertical(result_df, p_col='Enrichment_p', identifier_col='File_Identifier')\n",
    "        print('ssdsada')  # Display the first few rows of the combined DataFrame\n",
    "    # Bubble size based on the significance level (smaller p-values = larger bubbles)\n",
    "    df['Bubble_Size'] = (1 / df['FDR_Adjusted_p']) * 100  # Adjust scaling factor as needed\n",
    "    data = pd.read_csv(file_path)\n",
    "    data_cleaned = data.iloc[0:, [0, 2]]\n",
    "    merged_df = pd.merge(df, data_cleaned, left_on='File_Identifier', right_on='Cell subclass', how='left')\n",
    "    color_map = {cls: color for cls, color in zip(merged_df['Cell class'].unique(), plt.cm.tab20.colors)}\n",
    "    merged_df['Color'] = merged_df['Cell class'].map(color_map)\n",
    "    \n",
    "    # Create the vertical bubble chart\n",
    "    plt.figure(figsize=(10, 12))\n",
    "    plt.scatter(\n",
    "        merged_df['-log10(FDR_Adjusted_p)'],\n",
    "        merged_df['File_Identifier'],\n",
    "        s=merged_df['Bubble_Size'],\n",
    "        alpha=0.7,\n",
    "        edgecolors=\"w\",\n",
    "        c=merged_df['Color']\n",
    "    )\n",
    "\n",
    "    # Add plot decorations\n",
    "    plt.axvline(x=-np.log10(0.05), color='red', linestyle='--', linewidth=1, label='FDR threshold (0.05)')\n",
    "    plt.title('Bubble Chart of FDR Adjusted P-Values (Colored by Cell Class)', fontsize=14)\n",
    "    plt.xlabel('-log10(FDR Adjusted P-Values)', fontsize=12)\n",
    "    plt.ylabel('File Identifier (Cell Subclass)', fontsize=12)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.legend(handles=[plt.Line2D([0], [0], marker='o', color=color, label=cls, markersize=10, linestyle='None')\n",
    "                        for cls, color in color_map.items()], title='Cell Class', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "all_result=[]\n",
    "for i in range(128):\n",
    "    #print(i) \n",
    "    folder_path = \"/data/xzhao14/GWAS_output/QT\"+str(i)+\"_results/parh/QT\"+str(i)+\"_results.fastGWA\"  # Replace with your folder path\n",
    "    result_df = extract_first_row_with_pandas(folder_path,str(i))\n",
    "    all_result.append(result_df)\n",
    "   #plot_bubble_chart_vertical(result_df, p_col='Enrichment_p', identifier_col='File_Identifier')\n",
    "##\n",
    " # Display the first few rows of the combined DataFrame\n",
    "all_result = pd.concat(all_result, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "17e58"
   },
   "outputs": [],
   "source": [
    "corrected = multipletests(all_result['Enrichment_p'].values, alpha=0.05, method='fdr_bh')\n",
    "fdr_adjusted_p = corrected[1]\n",
    "all_result['Enrichment_p']=fdr_adjusted_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "a4030"
   },
   "outputs": [],
   "source": [
    "all_result[all_result['Enrichment_p']<0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "24779"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "def process_file(folder_path, file_suffix=\".results\", p_col=\"Enrichment_p\"):\n",
    "    \"\"\"\n",
    "    Process files in a folder to extract the specified p-value column and perform FDR correction.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): Path to the folder containing files.\n",
    "        file_suffix (str): File extension to process.\n",
    "        p_col (str): Column containing p-values.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A series with -log10(FDR_Adjusted_p) for each file.\n",
    "    \"\"\"\n",
    "    all_p_values = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(file_suffix):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep='\\t')\n",
    "                if p_col in df.columns:\n",
    "                    # Ensure p-values are numeric and replace NaN with 1\n",
    "                    p_values = pd.to_numeric(df[p_col], errors='coerce').fillna(1)\n",
    "                    all_p_values.append(p_values)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_name}: {e}\")\n",
    "    if all_p_values:\n",
    "        return pd.concat(all_p_values).reset_index(drop=True)\n",
    "    return pd.Series()\n",
    "\n",
    "def generate_matrix(base_folder_path, num_files=128, file_suffix=\".results\", p_col=\"Enrichment_p\"):\n",
    "    \"\"\"\n",
    "    Generate a 128x43 matrix with rows representing files and columns representing FDR-adjusted p-values.\n",
    "\n",
    "    Parameters:\n",
    "        base_folder_path (str): Path to the base folder containing all subfolders.\n",
    "        num_files (int): Number of subfolders to process.\n",
    "        file_suffix (str): File extension to process.\n",
    "        p_col (str): Column containing p-values.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A matrix with -log10(FDR_Adjusted_p) values.\n",
    "    \"\"\"\n",
    "    result_matrix = []\n",
    "    file_names = []\n",
    "    \n",
    "    for i in range(num_files):\n",
    "        folder_path = os.path.join(base_folder_path, f\"QT{i}_results/parh/QT{i}_results.fastGWA\")\n",
    "        print(f\"Processing folder: {folder_path}\")\n",
    "        \n",
    "        # Process files and extract p-values\n",
    "        p_values = process_file(folder_path, file_suffix=file_suffix, p_col=p_col)\n",
    "        \n",
    "        if not p_values.empty:\n",
    "            # Perform FDR correction\n",
    "            corrected = multipletests(p_values, alpha=0.05, method='fdr_bh')\n",
    "            fdr_adjusted_p = corrected[1]\n",
    "            \n",
    "            # Convert to -log10 scale\n",
    "            log_fdr_adjusted_p = -np.log10(fdr_adjusted_p)\n",
    "            log_fdr_adjusted_p[~np.isfinite(log_fdr_adjusted_p)] = 0  # Replace inf/-inf with 0\n",
    "            \n",
    "            result_matrix.append(log_fdr_adjusted_p[:43])  # Ensure 43 columns\n",
    "            file_names.append(f\"QT{i}\")\n",
    "        else:\n",
    "            print(f\"No valid p-values found in folder: QT{i}\")\n",
    "            result_matrix.append([np.nan] * 43)  # Fill with NaN if no data\n",
    "\n",
    "    # Create DataFrame\n",
    "    result_df = pd.DataFrame(result_matrix, index=file_names, columns=[f\"Feature_{j}\" for j in range(1, 44)])\n",
    "    return result_df\n",
    "\n",
    "# Base folder path\n",
    "base_folder_path = \"/data/xzhao14/GWAS_output\"\n",
    "\n",
    "# Generate the matrix\n",
    "matrix_df = generate_matrix(base_folder_path)\n",
    "\n",
    "# Save the matrix to a file (optional)\n",
    "# matrix_df.to_csv(\"/data/xzhao14/result_matrix.csv\", index=True)\n",
    "# print(\"Matrix generation complete. Saved to /data/xzhao14/result_matrix.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "e0970"
   },
   "outputs": [],
   "source": [
    "# All threshold #\n",
    "threshold = -np.log10(0.05 / 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "59cba"
   },
   "outputs": [],
   "source": [
    "matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "46348"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "file_path = \"/data/xzhao14/Supplemental_tables.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "data_cleaned = data.iloc[0:, [0, 2]]\n",
    "# matrix \"Cell subclass\"\n",
    "matrix_df.columns = data_cleaned[\"Cell subclass\"].values\n",
    "# p-value 0.05/128\n",
    "# define threshold\n",
    "threshold = -np.log10(0.05 / 128)\n",
    "#threshold=1.3\n",
    "# create a new matrix and replace values below threshold with 'X'\n",
    "annot_matrix = matrix_df.applymap(lambda x: \"X\" if x > threshold else \"\")\n",
    "\n",
    "# heatmapclass\n",
    "col_colors = data_cleaned.set_index(\"Cell subclass\")[\"Cell class\"]\n",
    "\n",
    "# map colors\n",
    "class_colors = {\n",
    "    \"GABA\": \"skyblue\",\n",
    "    \"GABA+Dopa\": \"orange\",\n",
    "    \"GLUT\": \"lightgreen\",\n",
    "    \"NonN\":\"yellow\"\n",
    "}\n",
    "col_colors_mapped = col_colors.map(class_colors)\n",
    "\n",
    "# plot heatmap\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(\n",
    "    matrix_df,\n",
    "    cmap=\"coolwarm\",  # colormap\n",
    "    cbar_kws={'label': 'Value'},  # \n",
    "    linewidths=0.5,  # \n",
    "    linecolor='black',  # \n",
    "    xticklabels=True,  # \n",
    "    yticklabels=True,  # \n",
    "    annot=annot_matrix,  # heatmap \"X\"\n",
    "    fmt=\"\",  # \n",
    "    square=False  # \n",
    ")\n",
    "\n",
    "# class\n",
    "for i, subclass in enumerate(matrix_df.columns):\n",
    "    cell_class = col_colors.loc[subclass]\n",
    "    plt.text(\n",
    "        i + 0.5, -5, cell_class, ha=\"center\", va=\"center\", fontsize=10, \n",
    "        rotation=90, color=class_colors[cell_class]\n",
    "    )\n",
    "\n",
    "# \n",
    "plt.title(\"Heatmap with Cell Class Annotations and Significant Markings\", fontsize=16)\n",
    "plt.xlabel(\"Cell Subclass\", fontsize=12)\n",
    "plt.ylabel(\"FA EP\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "f85b2"
   },
   "outputs": [],
   "source": [
    "### box plot ##\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# class\n",
    "anno_df=data_cleaned\n",
    "# compute enrichment grouped by cell class\n",
    "anno_df = anno_df.set_index(\"Cell subclass\")\n",
    "cell_classes = anno_df[\"Cell class\"]\n",
    "\n",
    "# matrixby\n",
    "\n",
    "# matrix\n",
    "matrix_long = matrix_df.melt(var_name=\"Cell Subclass\", value_name=\"Enrichment Value\")\n",
    "\n",
    "# map cell class information to each cell subclass\n",
    "#matrix_long[\"Cell Class\"] = matrix_long[\"Cell Subclass\"].map(cell_classes)\n",
    "\n",
    "# Enrichment Value > 3.408\n",
    "filtered_matrix_long = matrix_long[matrix_long[\"Enrichment Value\"] >3.48]\n",
    "\n",
    "# if no rows match, print a message\n",
    "\n",
    "# if no rows match, print a message\n",
    "if filtered_matrix_long.empty:\n",
    "    print(\"No Enrichment Values greater than 3.408 were found.\")\n",
    "else:\n",
    "    # Cell Subclass mean\n",
    "    mean_values = filtered_matrix_long.groupby(\"Cell Subclass\")[\"Enrichment Value\"].median().reset_index()\n",
    "\n",
    "    # by meansort\n",
    "    mean_values = mean_values.sort_values(by=\"Enrichment Value\", ascending=False)\n",
    "\n",
    "    # median_values = filtered_matrix_long.groupby(\"Cell Subclass\")[\"Enrichment Value\"].median().reset_index()\n",
    "\n",
    "    # # by mediansort\n",
    "    # median_values = median_values.sort_values(by=\"Enrichment Value\", ascending=False)\n",
    "\n",
    "    # barplot\n",
    "    plt.figure(figsize=(20, 8))\n",
    "   # ,\n",
    "    unique_classes = mean_values[\"Cell Subclass\"].nunique()  # class\n",
    "    custom_palette = sns.color_palette(\"husl\", unique_classes)  # HUSL\n",
    "\n",
    "    sns.barplot(\n",
    "        data=mean_values,\n",
    "        x=\"Cell Subclass\",\n",
    "        y=\"Enrichment Value\",\n",
    "        palette=custom_palette  #\n",
    "    )\n",
    "    plt.title(\" Enrichment of FA EP in Cell Subclasses\", fontsize=16)\n",
    "    plt.xlabel(\"Cell Subclass\", fontsize=12)\n",
    "    plt.ylabel(\"Enrichment(-log10(FDR)\", fontsize=12)\n",
    "    plt.xticks(rotation=90, fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "0028c"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "4ed48"
   },
   "outputs": [],
   "source": [
    "## MAGMA gene functions ##\n",
    "import os\n",
    "import pandas as pd\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "folder_path = \"/data484_2/xzhao14/FA_MAGMA_output\" \n",
    "# pathfile path\n",
    "#output_file = \"filtered_results.csv\"\n",
    "\n",
    "# initialize\n",
    "final_results = []\n",
    "\n",
    "# , gsa.out\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\"gsa.out\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        \n",
    "        # ,\n",
    "        try:\n",
    "            # read file content\n",
    "            with open(file_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            # \n",
    "            for idx, line in enumerate(lines):\n",
    "                if line.startswith(\"VARIABLE\"):\n",
    "                    header_idx = idx\n",
    "                    break\n",
    "\n",
    "            # \n",
    "            header = lines[header_idx].strip().split()\n",
    "            data = [line.strip().split() for line in lines[header_idx + 1:] if line.strip()]\n",
    "\n",
    "            # DataFrame\n",
    "            df = pd.DataFrame(data, columns=header)\n",
    "\n",
    "            data = df\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_name}: {e}\")\n",
    "            continue\n",
    "        #print(data)\n",
    "        # FULL_NAME\"GOB\"\n",
    "        filtered_data = data[data['FULL_NAME'].str.contains('REACTOME', na=False)]\n",
    "        \n",
    "        # ,\n",
    "        if filtered_data.empty:\n",
    "            continue\n",
    "        \n",
    "        # apply FDR correction to column P\n",
    "        # Pclass,\n",
    "        filtered_data['P'] = pd.to_numeric(filtered_data['P'], errors='coerce')\n",
    "\n",
    "        # Pconvert to\n",
    "        filtered_data = filtered_data.dropna(subset=['P'])\n",
    "\n",
    "        # ,\n",
    "        if filtered_data.empty:\n",
    "            continue\n",
    "\n",
    "        # apply FDR correction to column P\n",
    "        filtered_data['FDR'] = multipletests(filtered_data['P'], method='fdr_bh')[1]\n",
    "       # filtered_data['FDR'] = multipletests(filtered_data['P'], method='fdr_bh')[1]\n",
    "        \n",
    "        # FDR < 0.05/128\n",
    "        threshold = 0.01\n",
    "        significant_pathways = filtered_data[filtered_data['FDR'] < threshold]\n",
    "        \n",
    "        # ,\n",
    "        if significant_pathways.empty:\n",
    "            continue\n",
    "        \n",
    "        # save results\n",
    "        significant_pathways['Source_File'] = file_name\n",
    "        final_results.append(significant_pathways)\n",
    "\n",
    "# \n",
    "if final_results:\n",
    "    combined_results = pd.concat(final_results, ignore_index=True)\n",
    "#     combined_results.to_csv(output_file, index=False)\n",
    "#     print(f\"Filtered results saved to {output_file}\")\n",
    "# else:\n",
    "#     print(\"No significant pathways found in the given files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "2420a"
   },
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "df=combined_results\n",
    "# FDR -log10(FDR)\n",
    "df[\"-log10(FDR)\"] = -np.log10(df[\"FDR\"])\n",
    "\n",
    "df_sorted = df.sort_values(by=\"-log10(FDR)\", ascending=False)\n",
    "# sort\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(df_sorted[\"FULL_NAME\"], df_sorted[\"-log10(FDR)\"], color='skyblue')\n",
    "plt.xlabel(\"-log10(FDR)\", fontsize=12)\n",
    "plt.ylabel(\"FULL_NAME\", fontsize=12)\n",
    "#plt.title(\"Pathways and Their -log10(FDR) (Sorted)\", fontsize=14)\n",
    "plt.gca().invert_yaxis()  # Y ,\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "be040"
   },
   "source": [
    "# Get the FA related feature from the UKB table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "622b9"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "def filter_csv_by_columns(input_csv_path, column_list_path, output_csv_path, chunksize=100000):\n",
    "    \"\"\"\n",
    "    Filter columns from a large CSV file based on a list of column names or prefixes,\n",
    "    and save the filtered data to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv_path: str, path to the input CSV file\n",
    "    - column_list_path: str, path to the TXT file containing column names or prefixes (one per line)\n",
    "    - output_csv_path: str, path to save the filtered output CSV file\n",
    "    - chunksize: int, number of rows to read per chunk (default: 100000)\n",
    "    \"\"\"\n",
    "    # Read the list of column names or prefixes from the TXT file\n",
    "    with open(column_list_path, 'r') as f:\n",
    "        target_columns = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    # Initialize an empty DataFrame to store matched columns\n",
    "    filtered_data = pd.DataFrame()\n",
    "\n",
    "    # Iterate over chunks of the large CSV file\n",
    "    for chunk in pd.read_csv(input_csv_path, chunksize=chunksize):\n",
    "        # Find columns that match any target column or prefix\n",
    "        selected_columns = [col for col in chunk.columns if any(col.startswith(prefix) for prefix in target_columns)]\n",
    "        \n",
    "        if selected_columns:\n",
    "            filtered_chunk = chunk[selected_columns]\n",
    "            filtered_data = pd.concat([filtered_data, filtered_chunk], ignore_index=True)\n",
    "\n",
    "    filtered_data_clean = filtered_data.dropna(how='any')\n",
    "    # Save the filtered result to a new CSV file\n",
    "    filtered_data_clean.to_csv(output_csv_path, index=False)\n",
    "    ### EID#\n",
    "\n",
    "    ####\n",
    "    print(f\"Filtering completed. Output saved to: {output_csv_path}\")\n",
    "input_csv_path = '/data5/Ziqian/UKBB/UKB_data/UKB_all.csv'\n",
    "output_csv_path = '/data/xzhao14/FA_value.csv'\n",
    "request_txt='/data/xzhao14/UKB_request_FA.txt'\n",
    "filter_csv_by_columns(input_csv_path,request_txt,output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "f6288"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# file path\n",
    "input_csv_path = '/data5/Ziqian/UKBB/UKB_data/UKB_all.csv'\n",
    "output_csv_path = '/data/xzhao14/FA_disease_sample.csv'\n",
    "\n",
    "# chunksizememory\n",
    "chunksize = 100000  # \n",
    "\n",
    "# initialize DataFrame,\n",
    "filtered_data = pd.DataFrame()\n",
    "\n",
    "# \n",
    "for chunk in pd.read_csv(input_csv_path, chunksize=chunksize):\n",
    "    # '41270'\n",
    "    selected_columns = [col for col in chunk.columns if col.startswith('41270')]\n",
    "    if selected_columns:  # \n",
    "        filtered_chunk = chunk[selected_columns]\n",
    "        filtered_data = pd.concat([filtered_data, filtered_chunk], ignore_index=True)\n",
    "\n",
    "# CSV\n",
    "filtered_data.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"save result {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "4596f"
   },
   "outputs": [],
   "source": [
    "## AD sample choosing #\n",
    "import pandas as pd\n",
    "\n",
    "# input_csv_path = '/data5/Ziqian/UKBB/UKB_data/UKB_all.csv'\n",
    "#output_csv_path = '/data/xzhao14/FA_disease_sample.csv'\n",
    "# file path\n",
    "file1_path = '/data/xzhao14/FA_disease_sample.csv'  # CSVfile path\n",
    "file2_path = '/data5/Ziqian/UKBB/UKB_data/UKB_all.csv'  # CSVfile path\n",
    "output_path = '/data/xzhao14/UKB_sample_ICD10/Bone_abnormality_adult__sample.txt'  # file path\n",
    "\n",
    "# : 'G309' index\n",
    "df1 = pd.read_csv(file1_path)\n",
    "# 'G309',\n",
    "contains_g309 = df1.apply(lambda row: row.astype(str).str.contains('M83').any(), axis=1)\n",
    "indices_with_g309 = df1[contains_g309].index.tolist()\n",
    "\n",
    "# : 'eid'\n",
    "df2 = pd.read_csv(file2_path, usecols=['eid'])\n",
    "# index\n",
    "filtered_eid = df2.iloc[indices_with_g309]\n",
    "\n",
    "# :\n",
    "filtered_eid.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Filtering completed, results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "5e1df"
   },
   "outputs": [],
   "source": [
    "filtered_eid.to_csv('/data/xzhao14/AD_sample.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "e5dfc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# pathfile path\n",
    "folder_path = \"/data484_2/xzhao14/FA_rep_stat/\"  # path\n",
    "output_file = \"/data484_2/xzhao14/FA_replication5_8_sig_snp.txt\"  # output file\n",
    "\n",
    "# p-valuethreshold\n",
    "p_threshold = 5e-8\n",
    "\n",
    "# initialize DataFrame\n",
    "filtered_data = pd.DataFrame()\n",
    "\n",
    "# .fastGWA\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".fastGWA\"):\n",
    "        fea_name=file_name.split('_')[0]\n",
    "        print(fea_name)\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # read file content\n",
    "        try:\n",
    "            data = pd.read_csv(file_path, sep=\"\\t\")  # \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # p-value\n",
    "        if \"P\" not in data.columns:\n",
    "            print(f\"Skipping file (missing 'p' column): {file_name}\")\n",
    "            continue\n",
    "        \n",
    "        # p-valuethreshold\n",
    "        filtered_rows = data[data[\"P\"] < p_threshold]\n",
    "        \n",
    "        # ,\n",
    "        if not filtered_rows.empty:\n",
    "            filtered_rows[\"feature\"] = fea_name\n",
    "            # merge results\n",
    "            filtered_data = pd.concat([filtered_data, filtered_rows], ignore_index=True)\n",
    "\n",
    "# , txt\n",
    "if not filtered_data.empty:\n",
    "    filtered_data.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "    print(f\"Filtered data saved to: {output_file}\")\n",
    "else:\n",
    "    print(\"No rows passed the filtering criteria.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "395db"
   },
   "source": [
    "# Disorder analysis\n",
    "- genetic correlation\n",
    "- MR analysis\n",
    "- colocalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "e7bbd"
   },
   "source": [
    "## Genetic correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "1525f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import seaborn as sns\n",
    "\n",
    "def process_genetic_correlation_files(root_dir):\n",
    "    \"\"\"\n",
    "    Process genetic correlation files and extract relevant data.\n",
    "\n",
    "    Parameters:\n",
    "        root_dir (str): Path to the directory containing genetic correlation result files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame with relevant data including FDR values.\n",
    "    \"\"\"\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(root_dir)\n",
    "    data = []\n",
    "\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(root_dir, file_name)\n",
    "        with open(file_path, 'r') as f:\n",
    "            all_lines = f.readlines()\n",
    "            if len(all_lines) < 4:\n",
    "                print(f\"Warning: File {file_path} does not have enough lines.\")\n",
    "                continue\n",
    "            last_line = all_lines[-4].strip()  # Extract the fourth last line\n",
    "            data.append(last_line.split())  # Split the line into individual values and store in the list\n",
    "\n",
    "    # Define column names for the DataFrame\n",
    "    columns = ['P1', 'P2', 'rg', 'SE', 'Z', 'P', 'h2_obs', 'h2_obs_se', 'h2_int', 'h2_int_se', 'gcov_int', 'gcov_int_se']\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    # Extract clean names for P1 and P2\n",
    "    df['P1'] = df['P1'].str.extract(r'/([^/]+)\\.sumstats\\.gz')[0]\n",
    "    df['P2'] = df['P2'].str.extract(r'/([^/]+)\\.sumstats\\.gz')[0]\n",
    "    #\n",
    "    df['P1'] = df['P1'].str.split('_').str[0]\n",
    "  \n",
    "    # Replace \"NA\" with NaN and drop rows with NaN values\n",
    "    df.replace(\"NA\", np.nan, inplace=True)\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Convert relevant columns to numeric\n",
    "    df['rg'] = df['rg'].astype(float)\n",
    "    df['SE'] = df['SE'].astype(float)\n",
    "    df['P'] = df['P'].astype(float)\n",
    "\n",
    "    # Calculate FDR\n",
    "    df['FDR'] = multipletests(df['P'], alpha=0.05, method='fdr_bh')[1]\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_heatmap(df, output_file=None):\n",
    "    \"\"\"\n",
    "    Create a heatmap of FDR values with P1 as rows and P2 as columns.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing P1, P2, and FDR values.\n",
    "        output_file (str, optional): Path to save the heatmap image.\n",
    "    \"\"\"\n",
    "    # Pivot the data into a matrix\n",
    "    heatmap_data = df.pivot(index='P1', columns='P2', values='FDR')\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt=\".2e\", cmap='viridis', cbar_kws={'label': 'FDR'})\n",
    "    plt.title('Genetic Correlation Heatmap (FDR)')\n",
    "    plt.xlabel('P2')\n",
    "    plt.ylabel('P1')\n",
    "\n",
    "    if output_file:\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Heatmap saved to {output_file}\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    combined_df = pd.DataFrame()\n",
    "    for i in range(128):\n",
    "        root_dir = '/data/xzhao14/GWAS_output/QT'+str(i)+'_results/gc/QT'+str(i)+'_results.fastGWA'  # Replace with your directory\n",
    "        #output_file = \"genetic_correlation_heatmap.png\"  # Optional: Specify the output file for the heatmap\n",
    "        # Process files and create heatmap\n",
    "        df_gc = process_genetic_correlation_files(root_dir)\n",
    "        combined_df = pd.concat([combined_df, df_gc], ignore_index=True)\n",
    "   # print(df_gc)\n",
    "    #create_heatmap(df_gc, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "b4527"
   },
   "outputs": [],
   "source": [
    "#combined_df\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "combined_df['P1'] = combined_df['P1'].str.replace(r'^QT', 'EP', regex=True)\n",
    "combined_df_vio = combined_df.pivot(index='P1', columns='P2', values='rg')\n",
    "combined_df_vio=np.abs(combined_df_vio)\n",
    "colors = [\"blue\", \"white\", \"red\"]\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n",
    "\n",
    "# plot heatmap\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(combined_df_vio, annot=False, cmap=custom_cmap, cbar_kws={'label': 'rg'},\n",
    "            center=0, linewidths=0.5)\n",
    "plt.title('Genetic Correlation Heatmap (rg)')\n",
    "plt.xlabel('P2')\n",
    "plt.ylabel('P1')\n",
    "plt.tight_layout()\n",
    "# show heatmap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "5d6a3"
   },
   "outputs": [],
   "source": [
    "type(combined_df_vio)\n",
    "data=combined_df_vio\n",
    "col_means = data.median()\n",
    "col_stddevs = data.std()\n",
    "\n",
    "# by meansort\n",
    "sorted_indices = col_means.sort_values().index\n",
    "sorted_means = col_means[sorted_indices]\n",
    "sorted_stddevs = col_stddevs[sorted_indices]\n",
    "\n",
    "# \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(range(len(sorted_means)), sorted_means, yerr=sorted_stddevs, capsize=5)\n",
    "plt.xticks(range(len(sorted_means)), sorted_indices, rotation=45)\n",
    "plt.xlabel(\"Columns (sorted by mean)\")\n",
    "plt.ylabel(\"genetic correlation\")\n",
    "plt.title(\"Mean and Standard Deviation of Each Column\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "47ff7"
   },
   "source": [
    "### PRS information ## \n",
    "-  1. Clear up the GWAS summary statistic files \n",
    "-  2. Run the PRS-CS\n",
    "-  3. get the PRS score\n",
    "-  4. Using linear regression model to get the association between FA phenotype and PRS score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "ffdaf"
   },
   "outputs": [],
   "source": [
    "## read the GWAS files #\n",
    "import pandas as pd\n",
    "AD_file=pd.read_csv('/data484_2/xzhao14/heart/heart_disorder_GWAS/finngen_R12_Q17_COMPLEX_CARD_DEFEC',sep='\\t')\n",
    "df_selected = AD_file[['rsids', 'ref','alt','beta','sebeta']].rename(columns={'rsids':'SNP', 'ref':'A1','alt':'A2', 'beta':'BETA','sebeta':'SE'})\n",
    "# df_selected = AD_file[['SNP', 'A1','A2','OR','SE']]\n",
    "# df_selected\n",
    "# df_selected['A1'] = df_selected['A1'].str.upper()\n",
    "# df_selected['A2'] = df_selected['A2'].str.upper()\n",
    "df_selected.to_csv(\"/data484_2/xzhao14/heart/heart_disorder_GWAS/R12_Q17_COMPLEX_CARD_DEFEC.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "393e2"
   },
   "outputs": [],
   "source": [
    "df_selected.to_csv(\"/data484_2/xzhao14/heart/heart_disorder_GWAS/CARDIAC_ARRHYTM.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "35031"
   },
   "outputs": [],
   "source": [
    "# combine the PRS files\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Read all 22 PRS-CS files\n",
    "files = glob.glob(\"/data484_2/xzhao14/POST_GWAS/tools/PRScs/GWAS_novoerlap/PRS_output/HCM_pst_eff_a1_b0.5_phiauto_chr*.txt\")\n",
    "\n",
    "if not files:\n",
    "    print(\"⚠️ No PRS-CS result files found. Please check the path!\")\n",
    "    exit()\n",
    "\n",
    "df_list = []\n",
    "for f in files:\n",
    "    try:\n",
    "        # Read the file (no header in these files), so manually specify column names\n",
    "        df = pd.read_csv(f, sep='\\t', header=None)\n",
    "\n",
    "        # Ensure the file has at least 2 columns (SNP is in column 2)\n",
    "        if df.shape[1] < 2:\n",
    "            print(f\"⚠️ File {f} might be malformed – not enough columns. Skipping!\")\n",
    "            continue\n",
    "        \n",
    "        # Manually assign generic column names (assuming consistent format across files)\n",
    "        df.columns = [f\"col{i+1}\" for i in range(df.shape[1])]\n",
    "\n",
    "        # Keep only the SNP column (2nd column) and other data columns\n",
    "        df.rename(columns={'col2': 'SNP'}, inplace=True)\n",
    "\n",
    "        df_list.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error reading file {f}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Merge data\n",
    "if df_list:\n",
    "    merged_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    # Remove duplicates based on SNP column\n",
    "    merged_df.drop_duplicates(subset=['SNP'], inplace=True)\n",
    "    \n",
    "    # Save the merged file\n",
    "    output_file = \"/data484_2/xzhao14/POST_GWAS/tools/PRScs/disorder_prs/HCM_PRSCS.txt\"\n",
    "    merged_df.to_csv(output_file, sep=\" \", index=False, header=False)\n",
    "\n",
    "    print(f\"✅ PRS-CS results have been merged and saved to: {output_file}\")\n",
    "else:\n",
    "    print(\"⚠️ No PRS-CS result files were successfully read!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "9374e"
   },
   "source": [
    "## PRS linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "3679d"
   },
   "outputs": [],
   "source": [
    "## PRS linear regression\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def prs_linear_regression(pheno_file, covar_file, prs_file):\n",
    "    # Read data\n",
    "    pheno_df = pd.read_csv(pheno_file)\n",
    "    covar_df = pd.read_csv(covar_file, delim_whitespace=True)\n",
    "    prs_df = pd.read_csv(prs_file, delim_whitespace=True)\n",
    "    \n",
    "    # Rename the second column (IID) as ID for consistency\n",
    "    pheno_df = pheno_df.rename(columns={pheno_df.columns[1]: 'IID'})\n",
    "    covar_df = covar_df.rename(columns={covar_df.columns[1]: 'IID'})\n",
    "    prs_df = prs_df.rename(columns={prs_df.columns[1]: 'IID'})\n",
    "    \n",
    "    # Ensure IID is a string and remove duplicates and NaNs\n",
    "    for df in [pheno_df, covar_df, prs_df]:\n",
    "        df['IID'] = df['IID'].astype(str)\n",
    "        df.dropna(subset=['IID'], inplace=True)\n",
    "        df.drop_duplicates(subset=['IID'], inplace=True)\n",
    "    \n",
    "    # Find the intersection of IDs across all three files\n",
    "    common_ids = set(pheno_df['IID']) & set(covar_df['IID']) & set(prs_df['IID'])\n",
    "    print(common_ids)\n",
    "    \n",
    "    # Filter data to retain only common IDs\n",
    "    pheno_df = pheno_df[pheno_df['IID'].isin(common_ids)]\n",
    "    covar_df = covar_df[covar_df['IID'].isin(common_ids)]\n",
    "    prs_df = prs_df[prs_df['IID'].isin(common_ids)]\n",
    "    \n",
    "    # Merge data\n",
    "    merged_df = pheno_df.merge(covar_df, on='IID', suffixes=('_pheno', '_covar'))\n",
    "    merged_df = merged_df.merge(prs_df[['IID', 'SCORESUM']], on='IID')\n",
    "    \n",
    "    # Extract phenotype feature columns (excluding FID and IID)\n",
    "    pheno_features = merged_df.columns[2:len(pheno_df.columns)]\n",
    "    \n",
    "    # Extract covariate columns (excluding IID)\n",
    "    covar_features = merged_df.columns[len(pheno_df.columns):len(pheno_df.columns) + len(covar_df.columns) - 2]\n",
    "    \n",
    "    p_values = []  # Store p-values for each phenotype feature\n",
    "    results = {}\n",
    "    \n",
    "    # Perform linear regression for each phenotype feature\n",
    "    for feature in pheno_features:\n",
    "        Y = merged_df[feature]\n",
    "        X = merged_df[['SCORESUM'] + list(covar_features)]\n",
    "        X = sm.add_constant(X)  # Add intercept term\n",
    "        model = sm.OLS(Y, X).fit()\n",
    "        results[feature] = model.summary()\n",
    "        p_values.append(model.pvalues['SCORESUM'])\n",
    "    \n",
    "    # Print p-values and their mean\n",
    "    print(\"P-values for each phenotype feature:\", p_values)\n",
    "    print(\"Mean P-value:\", sum(p_values) / len(p_values))\n",
    "    \n",
    "    # Perform PCA on phenotype features\n",
    "    pca = PCA(n_components=2)\n",
    "    pheno_pca = pca.fit_transform(merged_df[pheno_features])\n",
    "    \n",
    "    # Perform regression with PCA components as phenotype\n",
    "    pca_results = {}\n",
    "    for i in range(2):\n",
    "        Y = pheno_pca[:, i]\n",
    "        X = merged_df[['SCORESUM'] + list(covar_features)]\n",
    "        X = sm.add_constant(X)  # Add intercept term\n",
    "        model = sm.OLS(Y, X).fit()\n",
    "        pca_results[f'PC{i+1}'] = model.summary()\n",
    "    \n",
    "    return results, pca_results\n",
    "#### Instance\n",
    "phenotype='/data/xzhao14/discovery_dti_128_pheno.csv'\n",
    "covar_file='/data/xzhao14/PRS_covar.csv'\n",
    "prs_file='/data484_2/xzhao14/POST_GWAS/tools/PRScs/disorder_prs/Heene_mineal_individual_results.profile'\n",
    "results, pca_results=prs_linear_regression(phenotype,covar_file,prs_file)\n",
    "print(pca_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "2960c"
   },
   "outputs": [],
   "source": [
    "prs_residual\n",
    "pheno_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "b31cb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "cocar=pd.read_csv('/data4012/zxie3/gcta/T1_ccovar_discovery_v2',delim_whitespace=True)\n",
    "qocar=pd.read_csv('/data4012/zxie3/gcta/T1_qcovar_discovery_v2',delim_whitespace=True)\n",
    "df_combined = pd.concat([cocar, qocar.iloc[:,2:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "c67ea"
   },
   "outputs": [],
   "source": [
    "##\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "all_feature=pd.read_csv('/data/xzhao14/FA_all_phenotype.csv')\n",
    "all_feature=all_feature.iloc[:,2:]\n",
    "corr_matrix = all_feature.corr()\n",
    "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# correlation coefficient ( NaN)\n",
    "mean_r = np.abs(upper_triangle.stack()).mean()\n",
    "std_r = np.abs(upper_triangle.stack()).std()\n",
    "print(f\"\\nAverage pairwise Pearson correlation (mean r): {mean_r:.4f}\")\n",
    "\n",
    "std_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "f9ede"
   },
   "outputs": [],
   "source": [
    "all_feature=all_feature[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "9d16d"
   },
   "outputs": [],
   "source": [
    "PRS_covar=df_combined[['FID', 'IID', 'PC0', 'PC1', 'PC2', 'PC3',\n",
    "       'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9','SEX', '54', 'AGE', 'AGE^2',\n",
    "       'SEXxAGE', 'SEXxAGE^2', '25000']]\n",
    "PRS_covar.to_csv('/data/xzhao14/PRS_covar.csv',sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "9d779"
   },
   "source": [
    "## multi-variant association between UDIP_FA and PRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "00676"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr, f\n",
    "import glob\n",
    "import os\n",
    "import warnings  \n",
    "def canoncorr_py(X, Y):\n",
    "    \"\"\"\n",
    "    Canonical Correlation Analysis (CCA) with -log10(P) for higher precision.\n",
    "\n",
    "    Parameters:\n",
    "    X : ndarray (n_samples, p)\n",
    "    Y : ndarray (n_samples, q)\n",
    "\n",
    "    Returns:\n",
    "    stats : dict\n",
    "        Contains F-statistic, p-value, -log10(p-value), canonical correlation coefficients\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    Y = np.asarray(Y)\n",
    "\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "    if Y.ndim == 1:\n",
    "        Y = Y.reshape(-1, 1)\n",
    "\n",
    "    # Standardize\n",
    "    X_std = StandardScaler().fit_transform(X)\n",
    "    Y_std = StandardScaler().fit_transform(Y)\n",
    "\n",
    "    n_components = min(X.shape[1], Y.shape[1])\n",
    "    cca = CCA(n_components=n_components)\n",
    "    U, V = cca.fit_transform(X_std, Y_std)\n",
    "\n",
    "    # Canonical correlations\n",
    "    r = [pearsonr(U[:, i], V[:, i])[0] for i in range(n_components)]\n",
    "\n",
    "    # Wilks' Lambda\n",
    "    wilks = np.prod([1 - ri**2 for ri in r])\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    q = Y.shape[1]\n",
    "    s = n_components\n",
    "\n",
    "    df1 = p * q\n",
    "    df2 = n - 1 - 0.5 * (p + q + 1)\n",
    "\n",
    "    # Approximate F-statistic\n",
    "    if s > 0:\n",
    "        wilks_root = wilks**(1 / s)\n",
    "        approx_F = ((1 - wilks_root) / wilks_root) * (df2 / df1)\n",
    "        p_value = f.sf(approx_F, df1, df2)  # Use survival function\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")  # Suppress warnings for log(0)\n",
    "            pF_log10 = -np.log10(p_value) if p_value > 0 else np.inf\n",
    "    else:\n",
    "        approx_F = np.nan\n",
    "        p_value = np.nan\n",
    "        pF_log10 = np.nan\n",
    "\n",
    "    stats = {\n",
    "        'F': approx_F,\n",
    "        'pF': p_value,\n",
    "        '-log10(pF)': pF_log10,\n",
    "        'r': r\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "def prs_linear_regression(pheno_file, covar_file, prs_file):\n",
    "    \"\"\"\n",
    "    Perform linear regression to obtain residuals for phenotype and PRS.\n",
    "    \n",
    "    Parameters:\n",
    "    pheno_file : str\n",
    "        Path to phenotype CSV file.\n",
    "    covar_file : str\n",
    "        Path to covariates CSV file (with whitespace delimiter).\n",
    "    prs_file : str\n",
    "        Path to PRS file (with whitespace delimiter).\n",
    "        \n",
    "    Returns:\n",
    "    pheno_resid : DataFrame\n",
    "        DataFrame of phenotype residuals (excluding IID column).\n",
    "    prs_resid : Series\n",
    "        Series of PRS residuals.\n",
    "    \"\"\"\n",
    "    # Read files\n",
    "    pheno_df = pd.read_csv(pheno_file)\n",
    "    covar_df = pd.read_csv(covar_file, delim_whitespace=True)\n",
    "    prs_df = pd.read_csv(prs_file, delim_whitespace=True)\n",
    "    \n",
    "    # Rename IID column to 'IID'\n",
    "    pheno_df = pheno_df.rename(columns={pheno_df.columns[1]: 'IID'})\n",
    "    covar_df = covar_df.rename(columns={covar_df.columns[1]: 'IID'})\n",
    "    prs_df = prs_df.rename(columns={prs_df.columns[1]: 'IID'})\n",
    "    \n",
    "    # Ensure IID is string and remove duplicates/missing values\n",
    "    for df in [pheno_df, covar_df, prs_df]:\n",
    "        df['IID'] = df['IID'].astype(str)\n",
    "        df.dropna(subset=['IID'], inplace=True)\n",
    "        df.drop_duplicates(subset=['IID'], inplace=True)\n",
    "    \n",
    "    # Keep common IIDs among the three files\n",
    "    common_ids = set(pheno_df['IID']) & set(covar_df['IID']) & set(prs_df['IID'])\n",
    "    print(f\"Number of common IDs in {prs_file}: {len(common_ids)}\")\n",
    "    \n",
    "    pheno_df = pheno_df[pheno_df['IID'].isin(common_ids)]\n",
    "    covar_df = covar_df[covar_df['IID'].isin(common_ids)]\n",
    "    prs_df = prs_df[prs_df['IID'].isin(common_ids)]\n",
    "    \n",
    "    # Merge data on IID\n",
    "    merged_df = pheno_df.merge(covar_df, on='IID', suffixes=('_pheno', '_covar'))\n",
    "    merged_df = merged_df.merge(prs_df[['IID', 'SCORESUM']], on='IID')\n",
    "    \n",
    "    # Determine phenotype features and covariate features based on column positions\n",
    "    pheno_features = merged_df.columns[2:len(pheno_df.columns)]\n",
    "    covar_features = merged_df.columns[len(pheno_df.columns):len(pheno_df.columns) + len(covar_df.columns) - 2]\n",
    "    \n",
    "    # Regress PRS on covariates to get residuals\n",
    "    X_covar = sm.add_constant(merged_df[covar_features])\n",
    "    prs_model = sm.OLS(merged_df['SCORESUM'], X_covar).fit()\n",
    "    prs_residual = prs_model.resid\n",
    "    merged_df['PRS_resid'] = prs_residual\n",
    "    \n",
    "    # Regress each phenotype on covariates to get residuals\n",
    "    pheno_resid_df = pd.DataFrame()\n",
    "    pheno_resid_df['IID'] = merged_df['IID']\n",
    "    \n",
    "    for feature in pheno_features:\n",
    "        Y = merged_df[feature]\n",
    "        X = sm.add_constant(merged_df[covar_features])\n",
    "        model = sm.OLS(Y, X).fit()\n",
    "        pheno_resid_df[feature + '_resid'] = model.resid\n",
    "\n",
    "    return pheno_resid_df.iloc[:, 1:], merged_df['PRS_resid']\n",
    "\n",
    "def process_all_prs_files(pheno_file, covar_file, prs_dir, output_file):\n",
    "    \"\"\"\n",
    "    Process all PRS files in a directory that end with 'individual_results.profile',\n",
    "    perform CCA between phenotype residuals and PRS residuals, and save the results as a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    pheno_file : str\n",
    "        Path to the phenotype CSV file.\n",
    "    covar_file : str\n",
    "        Path to the covariate CSV file.\n",
    "    prs_dir : str\n",
    "        Directory containing PRS files.\n",
    "    output_file : str\n",
    "        Path to the output CSV file where results will be saved.\n",
    "    \"\"\"\n",
    "    # Find all PRS files ending with 'individual_results.profile'\n",
    "    prs_files = glob.glob(os.path.join(prs_dir, '*individual_results.profile'))\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over each PRS file\n",
    "    for prs_file in prs_files:\n",
    "        try:\n",
    "            # Obtain phenotype and PRS residuals using linear regression\n",
    "            pheno_residuals, prs_residual = prs_linear_regression(pheno_file, covar_file, prs_file)\n",
    "            # Perform CCA between phenotype residuals and PRS residuals\n",
    "            stats = canoncorr_py(pheno_residuals, prs_residual)\n",
    "            # Since y is one-dimensional, the canonical correlation list should have one element\n",
    "            canonical_r = stats['r'][0] if stats['r'] else np.nan\n",
    "            # Append results\n",
    "            results.append({\n",
    "                'prs_file': os.path.basename(prs_file),\n",
    "                'F_statistic': stats['F'],\n",
    "                'p_value': stats['pF'],\n",
    "                'canonical_correlation': canonical_r\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {prs_file}: {e}\")\n",
    "            results.append({\n",
    "                'prs_file': os.path.basename(prs_file),\n",
    "                'F_statistic': np.nan,\n",
    "                'p_value': np.nan,\n",
    "                'canonical_correlation': np.nan,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Create a DataFrame from results and save to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "    return results_df\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    phenotype_file = '/data/xzhao14/discovery_dti_128_pheno.csv'\n",
    "    covar_file = '/data/xzhao14/PRS_covar.csv'\n",
    "    prs_directory = '/data484_2/xzhao14/POST_GWAS/tools/PRScs/disorder_prs'\n",
    "    output_csv = '/data/xzhao14/cca_FA_all_results.csv'\n",
    "    result_association=process_all_prs_files(phenotype_file, covar_file, prs_directory, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "de376"
   },
   "outputs": [],
   "source": [
    "# category name : 4283: Number of rounds of numeric memory test performed\t    20016: Fluid intelligence score 20018: \tProspective memory result 20023:\tMean time to correctly identify matches\n",
    "# 20139 : \tNumber of letters correctly identified\n",
    "cognitive_score_selected=cognitive_score[['eid','p4283_i2','p20016_i2','p20018_i2','p20023_i2','p20139_i2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "76000"
   },
   "source": [
    "### Phenotype association ### \n",
    "- associated with T1 and T2 \n",
    "- path: `/data4012/zxie3/MRI_AE_small_training_set_GWAS/T1_pheno_discovery`\n",
    "- build the linear regression model to get the association, adjust the covariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "75e17"
   },
   "outputs": [],
   "source": [
    "###\n",
    "import pandas as pd \n",
    "T1_pheno=pd.read_csv('/data4012/zxie3/MRI_AE_small_training_set_GWAS/T1_pheno_discovery',delim_whitespace=True)\n",
    "T2_pheno=pd.read_csv('/data4012/zxie3/MRI_AE_small_training_set_GWAS/T2_pheno_discovery',delim_whitespace=True)\n",
    "covar=pd.read_csv('/data4012/zxie3/MRI_AE_small_training_set_GWAS/T1_covar_discovery',delim_whitespace=True)\n",
    "FA_pheno=pd.read_csv('/data/xzhao14/discovery_dti_128_pheno.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "2a8a9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_blocked_heatmap(matrix, block_size=128, cmap=\"Blues\", output_pdf=\"heatmap.pdf\"):\n",
    "    \"\"\"\n",
    "plot the entire matrix on a single heatmap, add separators every block_size rows/columns, and save as a PDF.\n",
    "\n",
    "Parameters:\n",
    "- matrix: np.array pd.DataFrame, matrix\n",
    "- block_size: , default 128 ( 128 /)\n",
    "- cmap: colormap, default \"coolwarm\"\n",
    "- output_pdf: PDF path, default \"heatmap.pdf\"\n",
    "    \"\"\"\n",
    "    # if it's a NumPy array, convert to DataFrame= if isinstance(matrix, np.ndarray):\n",
    "        matrix = pd.DataFrame(matrix)\n",
    "\n",
    "    num_rows, num_cols = matrix.shape  # matrix\n",
    "\n",
    "    # create heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    ax = sns.heatmap(matrix, cmap=cmap, cbar=True, linewidths=0.5)\n",
    "\n",
    "    # ( 128 )\n",
    "    for i in range(block_size, num_rows, block_size):\n",
    "        ax.hlines(i, *ax.get_xlim(), colors=\"black\", linewidth=1.5, linestyles=\"dashed\")\n",
    "\n",
    "    # ( 128 )\n",
    "    for j in range(block_size, num_cols, block_size):\n",
    "        ax.vlines(j, *ax.get_ylim(), colors=\"black\", linewidth=1.5, linestyles=\"dashed\")\n",
    "\n",
    "    plt.title(\"Phenotype correlation Heatmap with Grid Separators\", fontsize=14)\n",
    "    plt.xlabel(\"Columns\", fontsize=12)\n",
    "    plt.ylabel(\"Rows\", fontsize=12)\n",
    "\n",
    "    # ** PDF**\n",
    "    plt.savefig(output_pdf, format=\"pdf\", bbox_inches=\"tight\")\n",
    "    plt.close()  # , memory\n",
    "    print(f\"✅ save {output_pdf}\")\n",
    "\n",
    "# **Example data**\n",
    "# matrix = np.random.rand(500, 500) # 500x500 randommatrix\n",
    "plot_blocked_heatmap(explained_variance_matrix, block_size=128, output_pdf=\"/data/xzhao14/FA_figures/phenotype_blocked_heatmap.pdf\")\n",
    "\n",
    "# **Example data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "a8444"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def compute_feature_explained_variance_cca(df1, df2, df3, covariate_df):\n",
    "    \"\"\"\n",
    "    Computes CCA-based explained variance between three datasets, adjusting for covariates.\n",
    "    \n",
    "    Parameters:\n",
    "        df1, df2, df3 (pd.DataFrame): DataFrames with IID and 128 features (QT0-QT127)\n",
    "        covariate_df (pd.DataFrame): DataFrame with IID and covariates\n",
    "    \n",
    "    Returns:\n",
    "        explained_variance_matrix (np.ndarray): 3x3 matrix of explained variance ratios\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Ensure IID is consistent across all dataframes\n",
    "    for df in [df1, df2, df3, covariate_df]:\n",
    "        df['IID'] = df['IID'].astype(str)\n",
    "        df.dropna(subset=['IID'], inplace=True)\n",
    "        df.drop_duplicates(subset=['IID'], inplace=True)\n",
    "\n",
    "    # Step 2: Align samples by common IID\n",
    "    common_iid = set(df1['IID']) & set(df2['IID']) & set(df3['IID']) & set(covariate_df['IID'])\n",
    "\n",
    "    def filter_by_common_ids(df):\n",
    "        return df[df['IID'].isin(common_iid)]\n",
    "    \n",
    "    df1, df2, df3, covariate_df = map(filter_by_common_ids, [df1, df2, df3, covariate_df])\n",
    "\n",
    "    # Step 3: Sort by IID to align rows\n",
    "    for df in [df1, df2, df3, covariate_df]:\n",
    "        df.sort_values('IID', inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Step 4: Extract numeric feature matrices and covariates\n",
    "    features_1 = df1.iloc[:, 1:].apply(pd.to_numeric, errors='coerce').values\n",
    "    features_2 = df2.iloc[:, 1:].apply(pd.to_numeric, errors='coerce').values\n",
    "    features_3 = df3.iloc[:, 1:].apply(pd.to_numeric, errors='coerce').values\n",
    "    covariates = covariate_df.iloc[:, 1:].apply(pd.to_numeric, errors='coerce').values\n",
    "\n",
    "    # Step 5: Add intercept to covariates\n",
    "    covariates = sm.add_constant(covariates)\n",
    "\n",
    "    # Step 6: Regress out covariates to get residuals\n",
    "    def compute_residuals(features, covariates):\n",
    "        residuals = np.zeros_like(features)\n",
    "        for i in range(features.shape[1]):\n",
    "            model = sm.OLS(features[:, i], covariates).fit()\n",
    "            residuals[:, i] = model.resid\n",
    "        return residuals\n",
    "\n",
    "    residuals_1 = compute_residuals(features_1, covariates)\n",
    "    residuals_2 = compute_residuals(features_2, covariates)\n",
    "    residuals_3 = compute_residuals(features_3, covariates)\n",
    "\n",
    "    # Step 7: Function to compute explained variance from X to Y and Y to X\n",
    "    def cca_explained_variance(X, Y):\n",
    "        # Perform SVD\n",
    "        U1, S1, _ = np.linalg.svd(X, full_matrices=False)  # U1: (n, 128), S1: (128,)\n",
    "        U2, S2, _ = np.linalg.svd(Y, full_matrices=False)  # U2: (n, 128), S2: (128,)\n",
    "\n",
    "        # SVD of U1.T @ U2\n",
    "        M = U1.T @ U2  # shape (128, 128)\n",
    "        U, S, Vt = np.linalg.svd(M, full_matrices=False)  # S: (128,)\n",
    "\n",
    "        # Compute explained variance\n",
    "        A = (S1[:, None] * U) * S  # shape (128, 128)\n",
    "        B = (S2[:, None] * Vt.T) * S  # shape (128, 128)\n",
    "\n",
    "        var_explained_X_by_Y = np.linalg.norm(A, 'fro')**2 / np.linalg.norm(S1, ord=2)**2\n",
    "        var_explained_Y_by_X = np.linalg.norm(B, 'fro')**2 / np.linalg.norm(S2, ord=2)**2\n",
    "\n",
    "        return var_explained_X_by_Y, var_explained_Y_by_X\n",
    "\n",
    "    # Step 8: Compute pairwise explained variance matrix\n",
    "    explained_variance_matrix = np.zeros((3, 3))\n",
    "\n",
    "    var_X_by_Y, var_Y_by_X = cca_explained_variance(residuals_1, residuals_2)\n",
    "    explained_variance_matrix[0, 1] = var_X_by_Y\n",
    "    explained_variance_matrix[1, 0] = var_Y_by_X\n",
    "\n",
    "    var_X_by_Z, var_Z_by_X = cca_explained_variance(residuals_1, residuals_3)\n",
    "    explained_variance_matrix[0, 2] = var_X_by_Z\n",
    "    explained_variance_matrix[2, 0] = var_Z_by_X\n",
    "\n",
    "    var_Y_by_Z, var_Z_by_Y = cca_explained_variance(residuals_2, residuals_3)\n",
    "    explained_variance_matrix[1, 2] = var_Y_by_Z\n",
    "    explained_variance_matrix[2, 1] = var_Z_by_Y\n",
    "\n",
    "    # Self-explained variance = 1.0\n",
    "    np.fill_diagonal(explained_variance_matrix, 1.0)\n",
    "\n",
    "    return explained_variance_matrix\n",
    "\n",
    "# Example data\n",
    "\n",
    "# CCA variance\n",
    "explained_variance_matrix=compute_feature_explained_variance_cca(T1_pheno,T2_pheno,FA_pheno,covar)\n",
    "\n",
    "# print(\" CCA variancematrix: \")\n",
    "# print(explained_variance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "acc8c"
   },
   "outputs": [],
   "source": [
    "print(explained_variance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "0d573"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def compute_feature_correlation(df1, df2, covariate_df):\n",
    "    \"\"\"\n",
    "DataFrame128correlation, the effects of covariates.\n",
    "\n",
    "Parameters:\n",
    "df1, df2, df3 (pd.DataFrame): DataFrame (IID) 128 (QT0-QT127)\n",
    "covariate_df (pd.DataFrame): (IID) covariates (2)\n",
    "\n",
    "Returns:\n",
    "tuple: (correlation_matrix, p_value_matrix) - 384×384 correlationmatrix p-valuematrix\n",
    "    \"\"\"\n",
    "\n",
    "    # **1. ensure IID , class**\n",
    "    for df in [df1, df2, df3, covariate_df]:\n",
    "        df['IID'] = df['IID'].astype(str)  # class\n",
    "        df.dropna(subset=['IID'], inplace=True)  # NaN\n",
    "        df.drop_duplicates(subset=['IID'], inplace=True)  # \n",
    "\n",
    "    # 4 DataFrame IID\n",
    "    common_iid = set(df1['IID']) & set(df2['IID']) & set(df3['IID']) & set(covariate_df['IID'])\n",
    "\n",
    "    # filter data\n",
    "    def filter_by_common_ids(df):\n",
    "        return df[df['IID'].isin(common_iid)]\n",
    "    ## \n",
    "    df1, df2, df3, covariate_df = map(filter_by_common_ids, [df1, df2, df3, covariate_df])\n",
    "    print(df1.shape)\n",
    "    # **4. by IID sort, ensure**\n",
    "    df1 = df1.sort_values('IID').reset_index(drop=True)\n",
    "    df2 = df2.sort_values('IID').reset_index(drop=True)\n",
    "    df3 = df3.sort_values('IID').reset_index(drop=True)\n",
    "    covariate_df = covariate_df.sort_values('IID').reset_index(drop=True)\n",
    "\n",
    "    # **5. **\n",
    "    merged_features = pd.concat([df1.iloc[:, 1:], df2.iloc[:, 1:], df3.iloc[:, 1:]], axis=1)  # (N, 384)\n",
    "\n",
    "    # **6. ensure all numeric columns are converted to `float` class, prevent `str` classerror**\n",
    "    merged_features = merged_features.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # **7. covariates**\n",
    "    feature_matrix = merged_features.iloc[:, 1:].values  # (N, 384) IID\n",
    "    covariates = covariate_df.iloc[:, 1:].apply(pd.to_numeric, errors='coerce').values  # (N, covariate_count) IID\n",
    "\n",
    "    # **8. ()covariatesmatrix**\n",
    "    covariates = sm.add_constant(covariates)\n",
    "\n",
    "    # **9. correlation ()**\n",
    "    num_features = feature_matrix.shape[1]\n",
    "    correlation_matrix = np.zeros((num_features, num_features))\n",
    "    p_value_matrix = np.zeros((num_features, num_features))\n",
    "    ##\n",
    "    for i in range(num_features):\n",
    "        for j in range(i, num_features):  # ,\n",
    "            # ** i **\n",
    "            model_i = sm.OLS(feature_matrix[:, i], covariates).fit()\n",
    "            residuals_i = model_i.resid\n",
    "\n",
    "            # ** j **\n",
    "            model_j = sm.OLS(feature_matrix[:, j], covariates).fit()\n",
    "            residuals_j = model_j.resid\n",
    "\n",
    "            # ** Pearson correlation**\n",
    "            corr, p_value = pearsonr(residuals_i, residuals_j)\n",
    "            correlation_matrix[i, j] = correlation_matrix[j, i] = corr\n",
    "            p_value_matrix[i, j] = p_value_matrix[j, i] = p_value\n",
    "\n",
    "    return correlation_matrix, p_value_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "bd340"
   },
   "outputs": [],
   "source": [
    "# Associated with cognitive functions ##\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr, f\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def canoncorr_py(X, Y):\n",
    "    \"\"\"\n",
    "    Python implementation of MATLAB's canoncorr function.\n",
    "    \n",
    "    Parameters:\n",
    "    X : ndarray (n_samples, p)\n",
    "        First variable group (e.g., predictors)\n",
    "    Y : ndarray (n_samples, q)\n",
    "        Second variable group (e.g., outcomes)\n",
    "        \n",
    "    Returns:\n",
    "    stats : dict\n",
    "        Dictionary with F statistic, p-value (Wilks' Lambda test),\n",
    "        and canonical correlation coefficients (list 'r')\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    Y = np.asarray(Y)\n",
    "    \n",
    "    # If the input is a one-dimensional array, convert it to a two-dimensional array\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "    if Y.ndim == 1:\n",
    "        Y = Y.reshape(-1, 1)\n",
    "\n",
    "    # Standardize X and Y\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_Y = StandardScaler()\n",
    "    X_std = scaler_X.fit_transform(X)\n",
    "    Y_std = scaler_Y.fit_transform(Y)\n",
    "\n",
    "    # Determine the number of canonical components (minimum number of columns in X and Y)\n",
    "    n_components = min(X.shape[1], Y.shape[1])\n",
    "\n",
    "    # Run Canonical Correlation Analysis (CCA)\n",
    "    cca = CCA(n_components=n_components)\n",
    "    U, V = cca.fit_transform(X_std, Y_std)\n",
    "    A = cca.x_weights_\n",
    "    B = cca.y_weights_\n",
    "\n",
    "    # Calculate canonical correlations for each component\n",
    "    r = [pearsonr(U[:, i], V[:, i])[0] for i in range(n_components)]\n",
    "\n",
    "    # Perform Wilks' Lambda test for overall significance\n",
    "    wilks = np.prod([1 - ri**2 for ri in r])\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    q = Y.shape[1]\n",
    "    s = n_components\n",
    "\n",
    "    df1 = p * q\n",
    "    df2 = n - 1 - 0.5 * (p + q + 1)\n",
    "    approx_F = ((1 - wilks**(1/s)) / (wilks**(1/s))) * (df2 / df1) if s > 0 else np.nan\n",
    "    p_value = 1 - f.cdf(approx_F, df1, df2) if s > 0 else np.nan\n",
    "\n",
    "    stats = {\n",
    "        'F': approx_F,\n",
    "        'pF': p_value,\n",
    "        'r': r\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "def prs_linear_regression(pheno_file, covar_file, prs_file):\n",
    "    \"\"\"\n",
    "    Perform linear regression to obtain residuals for phenotype and PRS.\n",
    "    \n",
    "    Parameters:\n",
    "    pheno_file : str\n",
    "        Path to phenotype CSV file.\n",
    "    covar_file : str\n",
    "        Path to covariates CSV file (with whitespace delimiter).\n",
    "    prs_file : str\n",
    "        Path to PRS file (with whitespace delimiter).\n",
    "        \n",
    "    Returns:\n",
    "    pheno_resid : DataFrame\n",
    "        DataFrame of phenotype residuals (excluding IID column).\n",
    "    prs_resid : Series\n",
    "        Series of PRS residuals.\n",
    "    \"\"\"\n",
    "    # Read files\n",
    "    pheno_df = pd.read_csv(pheno_file)\n",
    "    covar_df = pd.read_csv(covar_file, delim_whitespace=True)\n",
    "    prs_df = pd.read_csv(prs_file, sep='\\t')\n",
    "    \n",
    "    # Rename IID column to 'IID'\n",
    "    pheno_df = pheno_df.rename(columns={pheno_df.columns[1]: 'IID'})\n",
    "    covar_df = covar_df.rename(columns={covar_df.columns[1]: 'IID'})\n",
    "    prs_df = prs_df.rename(columns={prs_df.columns[1]: 'IID'})\n",
    "    \n",
    "    # Ensure IID is string and remove duplicates/missing values\n",
    "    for df in [pheno_df, covar_df, prs_df]:\n",
    "        df['IID'] = df['IID'].astype(str)\n",
    "        df.dropna(subset=['IID'], inplace=True)\n",
    "        df.drop_duplicates(subset=['IID'], inplace=True)\n",
    "    \n",
    "    # Keep common IIDs among the three files\n",
    "    common_ids = set(pheno_df['IID']) & set(covar_df['IID']) & set(prs_df['IID'])\n",
    "    print(f\"Number of common IDs in {prs_file}: {len(common_ids)}\")\n",
    "    \n",
    "    pheno_df = pheno_df[pheno_df['IID'].isin(common_ids)]\n",
    "    covar_df = covar_df[covar_df['IID'].isin(common_ids)]\n",
    "    prs_df = prs_df[prs_df['IID'].isin(common_ids)]\n",
    "    \n",
    "    # Merge data on IID\n",
    "    prs_colname=prs_df.columns\n",
    "    print(prs_colname)\n",
    "    merged_df = pheno_df.merge(covar_df, on='IID', suffixes=('_pheno', '_covar'))\n",
    "    merged_df = merged_df.merge(prs_df[['IID', prs_colname[2]]], on='IID')\n",
    "    \n",
    "    # Determine phenotype features and covariate features based on column positions\n",
    "    pheno_features = merged_df.columns[2:len(pheno_df.columns)]\n",
    "    covar_features = merged_df.columns[len(pheno_df.columns):len(pheno_df.columns) + len(covar_df.columns) - 2]\n",
    "    \n",
    "    # Regress PRS on covariates to get residuals\n",
    "    X_covar = sm.add_constant(merged_df[covar_features])\n",
    "    prs_model = sm.OLS(merged_df[prs_colname[2]], X_covar).fit()\n",
    "    prs_residual = prs_model.resid\n",
    "    merged_df['PRS_resid'] = prs_residual\n",
    "    \n",
    "    # Regress each phenotype on covariates to get residuals\n",
    "    pheno_resid_df = pd.DataFrame()\n",
    "    pheno_resid_df['IID'] = merged_df['IID']\n",
    "    \n",
    "    for feature in pheno_features:\n",
    "        Y = merged_df[feature]\n",
    "        X = sm.add_constant(merged_df[covar_features])\n",
    "        model = sm.OLS(Y, X).fit()\n",
    "        pheno_resid_df[feature + '_resid'] = model.resid\n",
    "\n",
    "    return pheno_resid_df.iloc[:, 1:], merged_df['PRS_resid']\n",
    "\n",
    "def process_all_prs_files(pheno_file, covar_file, prs_dir, output_file):\n",
    "    \"\"\"\n",
    "    Process all PRS files in a directory that end with 'individual_results.profile',\n",
    "    perform CCA between phenotype residuals and PRS residuals, and save the results as a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    pheno_file : str\n",
    "        Path to the phenotype CSV file.\n",
    "    covar_file : str\n",
    "        Path to the covariate CSV file.\n",
    "    prs_dir : str\n",
    "        Directory containing PRS files.\n",
    "    output_file : str\n",
    "        Path to the output CSV file where results will be saved.\n",
    "    \"\"\"\n",
    "    # Find all PRS files ending with 'individual_results.profile'\n",
    "    prs_files = glob.glob(os.path.join(prs_dir, '*.txt'))\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over each PRS file\n",
    "    for prs_file in prs_files:\n",
    "        try:\n",
    "            # Obtain phenotype and PRS residuals using linear regression\n",
    "            pheno_residuals, prs_residual = prs_linear_regression(pheno_file, covar_file, prs_file)\n",
    "            # Perform CCA between phenotype residuals and PRS residuals\n",
    "            stats = canoncorr_py(pheno_residuals, prs_residual)\n",
    "            # Since y is one-dimensional, the canonical correlation list should have one element\n",
    "            canonical_r = stats['r'][0] if stats['r'] else np.nan\n",
    "            # Append results\n",
    "            results.append({\n",
    "                'prs_file': os.path.basename(prs_file),\n",
    "                'F_statistic': stats['F'],\n",
    "                'p_value': stats['pF'],\n",
    "                'canonical_correlation': canonical_r\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {prs_file}: {e}\")\n",
    "            results.append({\n",
    "                'prs_file': os.path.basename(prs_file),\n",
    "                'F_statistic': np.nan,\n",
    "                'p_value': np.nan,\n",
    "                'canonical_correlation': np.nan,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Create a DataFrame from results and save to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "    return results_df\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    phenotype_file = '/data/xzhao14/discovery_dti_128_pheno.csv'\n",
    "    covar_file = '/data/xzhao14/PRS_covar.csv'\n",
    "    prs_directory = '/data484_2/xzhao14/cognitive_files'\n",
    "    output_csv = '/data/xzhao14/cca_FA_cog_results.csv'\n",
    "    result_association=process_all_prs_files(phenotype_file, covar_file, prs_directory, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "c622a"
   },
   "outputs": [],
   "source": [
    "# Association with cognitive score,include four cognitive subclass \n",
    "import pandas as pd\n",
    "import os\n",
    "cognitive_score=pd.read_csv('/data484_2/xzhao14/cognitive_phe.txt',sep='\\t')\n",
    "# category name : 4283: Number of rounds of numeric memory test performed\t    20016: Fluid intelligence score 20018: \tProspective memory result 20023:\tMean time to correctly identify matches\n",
    "# 20139 : \tNumber of letters correctly identified\n",
    "cognitive_score_selected=cognitive_score[['eid','p4283_i2','p20016_i2','p20018_i2','p20023_i2','p20139_i2']]\n",
    "#\n",
    "df=cognitive_score_selected\n",
    "# Add FID and IID columns based on 'eid'\n",
    "# Define the output directory\n",
    "output_dir = \"/data484_2/xzhao14/cognitive_files\"\n",
    "# Add FID and IID columns based on 'eid'\n",
    "df['FID'] = df['eid']\n",
    "df['IID'] = df['eid']\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Get protein columns (from column index 2 up to before FID/IID)\n",
    "# Adjust this if FID/IID are not the last columns\n",
    "protein_columns = df.columns[2:-2]\n",
    "\n",
    "# Loop through each protein column\n",
    "for col in protein_columns:\n",
    "    # Select FID, IID, and current protein column\n",
    "    sub_df = df[['FID', 'IID', col]].copy()\n",
    "    \n",
    "    # Drop rows with missing values in the current protein column\n",
    "    sub_df = sub_df.dropna(subset=[col])\n",
    "    \n",
    "    # Save to txt file if not empty\n",
    "    if not sub_df.empty:\n",
    "        output_path = os.path.join(output_dir, f\"{col}.txt\")\n",
    "        sub_df.to_csv(output_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "91cf2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# PRS\n",
    "prs_file = \"/data484_2/xzhao14/POST_GWAS/tools/PRScs/disorder_prs/SCZ_PRSCS.txt\"\n",
    "prs_df = pd.read_csv(prs_file, delim_whitespace=True, header=None)\n",
    "\n",
    "# SNP ID 1\n",
    "prs_df.rename(columns={1: 'SNP'}, inplace=True)\n",
    "\n",
    "# statistics SNP\n",
    "duplicate_snps = prs_df['SNP'].value_counts()\n",
    "duplicate_snps = duplicate_snps[duplicate_snps > 1]\n",
    "\n",
    "if not duplicate_snps.empty:\n",
    "    print(f\"⚠️ PRS file contains {len(duplicate_snps)} duplicate SNPs, for example:\")\n",
    "    print(duplicate_snps.head(10))\n",
    "\n",
    "    # SNP\n",
    "    duplicate_snps.index.to_series().to_csv(\"/data484_2/xzhao14/POST_GWAS/tools/PRScs/disorder_prs/duplicate_snps_from_prs.txt\", index=False, header=False)\n",
    "\n",
    "    # drop duplicates SNP\n",
    "    prs_df.drop_duplicates(subset=['SNP'], inplace=True)\n",
    "\n",
    "    # drop duplicates PRS\n",
    "    output_file = \"/data484_2/xzhao14/POST_GWAS/tools/PRScs/disorder_prs/SCZ_PRSCS_filtered.txt\"\n",
    "    prs_df.to_csv(output_file, sep=\" \", index=False, header=False)\n",
    "    \n",
    "    print(f\"✅ PRS results after removing duplicate SNPs saved to:{output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "1f29f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sampple_data=pd.read_csv('/data484_2/xzhao14/FA_statis/QT108_results.fastGWA',sep='\\t')\n",
    "meta_path='/data484_2/xzhao14/POST_GWAS/tools/generic-metal/meta/QT411.tbl'\n",
    "data=pd.read_csv(meta_path,sep='\\t')\n",
    "match = re.search(r'/([^/]+)\\d\\.tbl$', meta_path)\n",
    "result = match.group(1)\n",
    "df1_ordered = sampple_data.set_index('SNP').loc[data['MarkerName']].reset_index()\n",
    "data['CHR']=df1_ordered['CHR']\n",
    "data['POS']=df1_ordered['POS']\n",
    "data['A1']=df1_ordered['A1']\n",
    "data['A2']=df1_ordered['A2']\n",
    "data['N']=df1_ordered['N']\n",
    "data['AF1']=df1_ordered['AF1']\n",
    "df_selected = data[['MarkerName','CHR', 'A1','A2','AF1','Effect','StdErr','P-value','N']].rename(columns={'MarkerName': 'SNP', 'Effect': 'BETA','StdErr':'SE','P-value':'P'})\n",
    "our_sample=result+'_FA.txt'\n",
    "#####\n",
    "df_selected.to_csv(result,sep='\\t',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "0b5dc"
   },
   "outputs": [],
   "source": [
    "meta_path='/data484_2/xzhao14/POST_GWAS/tools/generic-metal/meta/QT411.tbl'\n",
    "match_d = re.search(r'/([^/]+)\\d\\.tbl$', meta_path)\n",
    "result = match_d.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "65ad4"
   },
   "outputs": [],
   "source": [
    "##\n",
    "import pandas as pd\n",
    "dd=pd.read_csv('/data484_1/kmohammed2/anaseri_bgen_processing/ccovar',delim_whitespace=True)\n",
    "dd_se=dd[['FID','IID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "1763c"
   },
   "outputs": [],
   "source": [
    "dd_se.to_csv('/data484_2/xzhao14/heart/6w_sample.txt',index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "a7b16"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def manhattan_plot(data, significance_threshold=5e-8, suggestive_threshold=1e-5, title=\"Manhattan Plot\"):\n",
    "    \"\"\"\n",
    "    Draw a Manhattan plot.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame with columns \"CHR\", \"POS\", \"P\"\n",
    "    - significance_threshold: genome-wide significance threshold (default 5e-8)\n",
    "    - suggestive_threshold: suggestive threshold (default 1e-5)\n",
    "    - title: plot title\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure numeric data types\n",
    "    data[\"CHR\"] = data[\"CHR\"].astype(int)\n",
    "    data[\"BP\"] = data[\"POS\"].astype(int)\n",
    "    data[\"-log10(P)\"] = -np.log10(data[\"P\"].astype(float).clip(lower=1e-300))\n",
    "\n",
    "    # Sort by chromosome and position\n",
    "    data = data.sort_values([\"CHR\", \"BP\"])\n",
    "\n",
    "    # Create cumulative index for plotting\n",
    "    data[\"ind\"] = range(len(data))\n",
    "    chrom_df = data.groupby(\"CHR\")[\"ind\"].median()\n",
    "\n",
    "    # Assign colors based on chromosome\n",
    "    colors = [\"#4daf4a\", \"#377eb8\"]\n",
    "    color_list = [colors[i % 2] for i in data[\"CHR\"]]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(data[\"ind\"], data[\"-log10(P)\"], c=color_list, alpha=0.6, s=10)\n",
    "\n",
    "    # Add threshold lines\n",
    "    plt.axhline(y=-np.log10(significance_threshold), color=\"red\", linestyle=\"--\", label=f\"Genome-wide: 5e-8\")\n",
    "    plt.axhline(y=-np.log10(suggestive_threshold), color=\"blue\", linestyle=\"--\", label=f\"Suggestive: 1e-5\")\n",
    "\n",
    "    # Customize axes\n",
    "    plt.xticks(ticks=chrom_df.values, labels=chrom_df.index, fontsize=10)\n",
    "    plt.xlabel(\"Chromosome\", fontsize=12)\n",
    "    plt.ylabel(\"-log10(P-value)\", fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.legend()\n",
    "\n",
    "    # Grid and layout\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load your GWAS result\n",
    "data = pd.read_csv('/data484_2/xzhao14/heart/HEART_2ich_inputation_3D_GAN.txt', sep='\\t')\n",
    "selected_data=data[data['P']<5e-8]\n",
    "print(selected_data.shape)\n",
    "# Run the plot\n",
    "#manhattan_plot(data, title=\"4ch_3D_GAN_Manhattan Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "de85e"
   },
   "outputs": [],
   "source": [
    "#\n",
    "import os\n",
    "f2='/data484_2/xzhao14/FA_meta/ldsc_sumstats/QT32_FA.sumstats.sumstats.gz'\n",
    "os.path.basename(f2).split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "c8c65"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('/data484_1/kmohammed2/anaseri_bgen_processing/gcta/ukb_grm.grm.id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "9bde4"
   },
   "source": [
    "## Upload to GWAS Catalog\n",
    "\n",
    "Here is the script for formatting the summary statistics to conform with GWAS Catalog requirements and generating the description table, you then need to use the [globus client](https://www.globus.org/globus-connect-personal) to upload the summary statistics to the network folder designated by GWAS Catalog team:\n",
    "```Python\n",
    "# https://www.ebi.ac.uk/gwas/docs/submission\n",
    "# https://www.ebi.ac.uk/gwas/docs/summary-statistics-format#validator\n",
    "# https://www.ebi.ac.uk/gwas/deposition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the files \n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "def rename_qt_to_udip(folder: str, dry_run: bool = False) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Rename files matching 'QT<number>_results.fastGWA' to 'UDIP<number>.fastGWA' in the specified folder.\n",
    "\n",
    "    Args:\n",
    "        folder: Path to the directory containing the files.\n",
    "        dry_run: If True, do not perform actual renaming; just print planned changes.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples (original_path, new_path) for files that were (or would be) renamed.\n",
    "    \"\"\"\n",
    "    dir_path = Path(folder)\n",
    "    if not dir_path.is_dir():\n",
    "        raise ValueError(f\"Provided path is not a directory: {folder}\")\n",
    "\n",
    "    # Regex to capture the numeric part in filenames like QT0_results.fastGWA or QT10_results.fastGWA\n",
    "    pattern = re.compile(r'^QT(\\d+)_results\\.fastGWA$', flags=re.IGNORECASE)\n",
    "    renamed = []\n",
    "\n",
    "    for entry in dir_path.iterdir():\n",
    "        if not entry.is_file():\n",
    "            continue\n",
    "        m = pattern.match(entry.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        idx = m.group(1)\n",
    "        new_name = f\"UDIP{idx}.fastGWA\"\n",
    "        target = entry.with_name(new_name)\n",
    "\n",
    "        if target.exists():\n",
    "            print(f\"[SKIP] Target already exists, skipping rename of '{entry.name}' -> '{new_name}'\")\n",
    "            continue\n",
    "\n",
    "        if dry_run:\n",
    "            print(f\"[DRY-RUN] Would rename: '{entry.name}' -> '{new_name}'\")\n",
    "        else:\n",
    "            entry.rename(target)\n",
    "            print(f\"[RENAMED] '{entry.name}' -> '{new_name}'\")\n",
    "\n",
    "        renamed.append((str(entry), str(target)))\n",
    "\n",
    "    return renamed\n",
    "\n",
    "\n",
    "folder='/data484_2/xzhao14/FA_rep_stat/'\n",
    "results = rename_qt_to_udip(folder, dry_run=False)\n",
    "print(f\"Processed {len(results)} file(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, os\n",
    "from glob import glob\n",
    "from gwas_sumstats_tools.validate import validate\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "validator_path = \"/data4012/zxie3/anaconda3/envs/workflow/lib/python3.9/site-packages/gwas_sumstats_tools/validate.py\"\n",
    "## \n",
    "files = glob(\"/data484_2/xzhao14/FA_statis/UDIP*.fastGWA\")\n",
    "columns = [\"chromosome\", \"base_pair_location\", \"effect_allele\", \"other_allele\", \"beta\", \"standard_error\", \"effect_allele_frequency\", \"p_value\"]\n",
    "mapping = {\"CHR\":\"chromosome\", \"POS\":\"base_pair_location\", \"A1\":\"effect_allele\", \"A2\":\"other_allele\", \"BETA\":\"beta\", \"SE\":\"standard_error\", \"AF1\":\"effect_allele_frequency\", \"P\":\"p_value\"}\n",
    "\n",
    "def parse_name(x):\n",
    "    modality, model, qt, cohort = x.split('/')[-1].split('_')\n",
    "    cohort = cohort.split('.')[0]\n",
    "    return f\"{modality}_{qt}_{cohort}\"\n",
    "\n",
    "def proc(x, path):\n",
    "    df = pd.read_table(x)\n",
    "    df.rename(columns=mapping)[columns].to_csv(path, sep='\\t', index=False)\n",
    "\n",
    "with Pool(5) as p:\n",
    "    p.starmap(proc, tqdm.tqdm(zip(files[1:], map(lambda x: f\"/data484_2/xzhao14/gwascatalog_submission/{parse_name(x)}.tsv\", files[1:]))))\n",
    "# validate(f\"/data4012/zxie3/AE_revision2/gwascatalog_submission/{parse_name(files[0])}.tsv\")\n",
    "# https://www.ebi.ac.uk/gwas/docs/submission-summary-statistics-plus-metadata\n",
    "os.system(\"md5sum /data484_2/xzhao14/gwascatalog_submission*.tsv > /data484_2/xzhao14/FA_checksum\")\n",
    "md5_df = pd.read_table(\"/data484_2/xzhao14/FA_checksum\", header=None, sep='\\s+')\n",
    "Study_tag = \"MRI_FA_GWAS_MODEL128_{}_UDIP{}_{}\" #A5 modality T1/T2, dimension and discovery/replication\n",
    "Genotyping_technology = \"Genome-wide genotyping array\" #B5\n",
    "Array_manufacturer = \"Affymetrix\" #C5\n",
    "Analysis_software = \"GCTA fastGWA\" #E5\n",
    "Imputation = \"Yes\" #F5\n",
    "Reported_trait = \"Algorithmically Discovered Brain Morphology-Related Trait\" #M5\n",
    "Summary_statistics_file  = \"{}_QT{}_{}.tsv\" #O5 modality T1/T2, dimension and discovery/replication\n",
    "md5_sum = \"\" #P5 md5\n",
    "citation = 'Patel, Khush, et al. \"New phenotype discovery method by unsupervised deep representation learning empowers genetic association studies of brain imaging.\" medRxiv (2022): 2022-12.'\n",
    "Readme = citation #Q5\n",
    "Summary_statistics_assembly = \"GRCh37\"  #R5\n",
    "Coordinate_system = \"1-based\"  #Z5\n",
    "Stage = \"discovery\" # discovery/replication\n",
    "Ancestry_category = \"European\"\n",
    "Ancestry = \"British White\"\n",
    "Country_of_recruitment = \"U.K.\"\n",
    "Variant_count_dict = {(\"FA\", \"discovery\"): 8925988, (\"FA\", \"replication\"): 8925870}\n",
    "Number_of_individuals_dict = {(\"FA\", \"discovery\"): 22960, (\"FA\", \"replication\"): 12405}\n",
    "workbook = load_workbook('/data4012/zxie3/AE_revision2/new_template.xlsx')\n",
    "study  = workbook['study']\n",
    "sample = workbook['sample']\n",
    "# modality = 'T1'\n",
    "# dim = 0\n",
    "# cohort = \"discovery\"\n",
    "line = 5  # start from 5th line in the excel sheet\n",
    "for modality in [\"T1\", \"T2\"]:\n",
    "    for cohort in [\"discovery\", \"replication\"]:\n",
    "        for dim in range(128):\n",
    "            summary_statistics_file = Summary_statistics_file.format(modality, dim, cohort)\n",
    "            if not os.path.exists(f\"gwascatalog_submission/{summary_statistics_file}\"):\n",
    "                print(summary_statistics_file)\n",
    "                continue\n",
    "            study_tag = Study_tag.format(modality, dim, cohort)\n",
    "            variant_count = Variant_count_dict[(modality, cohort)]\n",
    "\n",
    "            md5 = md5_df[md5_df[1] == f\"gwascatalog_submission/{summary_statistics_file}\"][0].values[0]\n",
    "            stage = cohort\n",
    "            number_of_individuals = Number_of_individuals_dict[(modality, cohort)]\n",
    "            study_filling = [study_tag, Genotyping_technology, Array_manufacturer, Analysis_software, Imputation, variant_count,\n",
    "                             Reported_trait, summary_statistics_file, md5, Readme, Summary_statistics_assembly, Coordinate_system]\n",
    "            study_filling_cell = [f'{x}{line}' for x in ['A', 'B', 'C', 'E', 'F', 'I', 'M', 'O', 'P', 'Q', 'R', 'Z']]\n",
    "            for (cell, content) in zip(study_filling_cell, study_filling):\n",
    "                study[cell] = content\n",
    "            sample_filling = [study_tag, stage, number_of_individuals, Ancestry_category, Ancestry, Country_of_recruitment]\n",
    "            sample_filling_cell = [f'{x}{line}' for x in ['A', 'B', 'C', 'H', 'I', 'L']]\n",
    "            for (cell, content) in zip(sample_filling_cell, sample_filling):\n",
    "                sample[cell] = content\n",
    "            line += 1\n",
    "workbook.save(f'/data4012/zxie3/AE_revision2/GWASCatalog_submission_form/all.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "post_gwas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vincent": {
   "sessionId": "2e903896c725696aa970b764_2025-05-30T16-56-03-891Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}